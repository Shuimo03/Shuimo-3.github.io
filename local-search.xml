<?xml version="1.0" encoding="utf-8"?>
<search>
  
  
  
  <entry>
    <title>K8S节点NotReady排错思路</title>
    <link href="/2024/01/14/K8S%E8%8A%82%E7%82%B9NotReady%E6%8E%92%E9%94%99%E6%80%9D%E8%B7%AF/"/>
    <url>/2024/01/14/K8S%E8%8A%82%E7%82%B9NotReady%E6%8E%92%E9%94%99%E6%80%9D%E8%B7%AF/</url>
    
    <content type="html"><![CDATA[<h2 id="背景"><a href="#背景" class="headerlink" title="背景"></a>背景</h2><p>在K8S中这种问题节点出现问题导致NotReady的情况并不少见，比如资源不足导致节点不可用，这里简单介绍下我遇到的情况以及我是如何解决的。</p><h2 id="排查思路"><a href="#排查思路" class="headerlink" title="排查思路"></a>排查思路</h2><p>我使用的是VMware虚拟化出来的三个节点，一个master两个node，在kubectl get node的时候发现节点不可用:</p><p><img src="/../images/K8S/image-20240114182639915.png" alt="image-20240114182639915"></p><p>以往一般会直接去重启对应节点机器，但是集群是重启过的，进一步查看原因，使用kubectl describe node node1</p><p><img src="/../images/K8S/image-20240114182853476.png" alt="image-20240114182853476"></p><p>可以看到kublet启动了，并且上报了对应节点状态，接着看到kube-proxy出现了问题，所以将思路聚焦在节点网络上:</p><p><img src="/../images/K8S/image-20240114183016235.png" alt="image-20240114183016235"></p><p>节点上网卡并没有启动，所以只需要启动网卡即可。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs bash">nmcli networking on<br>nmcli connection up ens160 <span class="hljs-comment">#这里替换成你的网卡名称</span><br></code></pre></td></tr></table></figure><p><img src="/../images/K8S/image-20240114183223918.png" alt="image-20240114183223918"></p><p><img src="/../images/K8S/image-20240114183255962.png" alt="image-20240114183255962"></p><p>问题解决，最后一个node2同样的排查思路。</p>]]></content>
    
    
    
    <tags>
      
      <tag>kubernetes</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>client-go组件使用与原理</title>
    <link href="/2024/01/12/client-go%E7%BB%84%E4%BB%B6%E4%BD%BF%E7%94%A8%E4%B8%8E%E5%8E%9F%E7%90%86/"/>
    <url>/2024/01/12/client-go%E7%BB%84%E4%BB%B6%E4%BD%BF%E7%94%A8%E4%B8%8E%E5%8E%9F%E7%90%86/</url>
    
    <content type="html"><![CDATA[<h2 id="背景"><a href="#背景" class="headerlink" title="背景"></a>背景</h2><p>之前大部分时间都在用client-go进行开发，也用过informer做一些有意思的事情，但是很多细节方面的东西没有注意，这次算是对client-go原理进行一个整理。</p><h2 id="架构设计"><a href="#架构设计" class="headerlink" title="架构设计"></a>架构设计</h2><h2 id="client-go类型"><a href="#client-go类型" class="headerlink" title="client-go类型"></a>client-go类型</h2><p>client-go支持四种类型客户端对象来和api server进行交互:</p><ul><li>RESTClient</li><li>ClientSet</li><li>DynamicClient</li><li>DiscoveryClient</li></ul><p>其他三种都是基于RESTClient实现，所以RESTClient可以算是他们的父类。</p><p><img src="/../images/K8S/image-20240115094702534.png" alt="image-20240115094702534"></p><h3 id="RESTClient"><a href="#RESTClient" class="headerlink" title="RESTClient"></a>RESTClient</h3><p>RESTClient是最基础客户端，对HTTP Request进行封装，实现RESTful API。</p><figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><code class="hljs go"><span class="hljs-comment">// Interface captures the set of operations for generically interacting with Kubernetes REST apis.</span><br><span class="hljs-keyword">type</span> Interface <span class="hljs-keyword">interface</span> &#123;<br>GetRateLimiter() flowcontrol.RateLimiter<br>Verb(verb <span class="hljs-type">string</span>) *Request<br>Post() *Request<br>Put() *Request<br>Patch(pt types.PatchType) *Request<br>Get() *Request<br>Delete() *Request<br>APIVersion() schema.GroupVersion<br>&#125;<br></code></pre></td></tr></table></figure><h3 id="ClientSet"><a href="#ClientSet" class="headerlink" title="ClientSet"></a>ClientSet</h3><p>ClientSet在RESTClient基础上添加了Resouce和Version的管理方法，每一个Resouce都是一个客户端，ClientSet就是这些客户端的集合，通过函数来暴露这些Resource和Version，需要注意的是ClienSet只支持K8S内置资源，如果需要对CRD资源进行访问会特别麻烦，还需要通过client-gen重新生成ClientSet。意义不大。</p><figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br></pre></td><td class="code"><pre><code class="hljs go"><span class="hljs-keyword">type</span> Interface <span class="hljs-keyword">interface</span> &#123;<br>Discovery() discovery.DiscoveryInterface<br>AdmissionregistrationV1() admissionregistrationv1.AdmissionregistrationV1Interface<br>AdmissionregistrationV1alpha1() admissionregistrationv1alpha1.AdmissionregistrationV1alpha1Interface<br>AdmissionregistrationV1beta1() admissionregistrationv1beta1.AdmissionregistrationV1beta1Interface<br>InternalV1alpha1() internalv1alpha1.InternalV1alpha1Interface<br>AppsV1() appsv1.AppsV1Interface<br>AppsV1beta1() appsv1beta1.AppsV1beta1Interface<br>AppsV1beta2() appsv1beta2.AppsV1beta2Interface<br>AuthenticationV1() authenticationv1.AuthenticationV1Interface<br>AuthenticationV1alpha1() authenticationv1alpha1.AuthenticationV1alpha1Interface<br>AuthenticationV1beta1() authenticationv1beta1.AuthenticationV1beta1Interface<br>AuthorizationV1() authorizationv1.AuthorizationV1Interface<br>AuthorizationV1beta1() authorizationv1beta1.AuthorizationV1beta1Interface<br>AutoscalingV1() autoscalingv1.AutoscalingV1Interface<br>AutoscalingV2() autoscalingv2.AutoscalingV2Interface<br>AutoscalingV2beta1() autoscalingv2beta1.AutoscalingV2beta1Interface<br>AutoscalingV2beta2() autoscalingv2beta2.AutoscalingV2beta2Interface<br>BatchV1() batchv1.BatchV1Interface<br>BatchV1beta1() batchv1beta1.BatchV1beta1Interface<br>CertificatesV1() certificatesv1.CertificatesV1Interface<br>CertificatesV1beta1() certificatesv1beta1.CertificatesV1beta1Interface<br>CertificatesV1alpha1() certificatesv1alpha1.CertificatesV1alpha1Interface<br>CoordinationV1beta1() coordinationv1beta1.CoordinationV1beta1Interface<br>CoordinationV1() coordinationv1.CoordinationV1Interface<br>CoreV1() corev1.CoreV1Interface<br>DiscoveryV1() discoveryv1.DiscoveryV1Interface<br>DiscoveryV1beta1() discoveryv1beta1.DiscoveryV1beta1Interface<br>EventsV1() eventsv1.EventsV1Interface<br>EventsV1beta1() eventsv1beta1.EventsV1beta1Interface<br>ExtensionsV1beta1() extensionsv1beta1.ExtensionsV1beta1Interface<br>FlowcontrolV1() flowcontrolv1.FlowcontrolV1Interface<br>FlowcontrolV1beta1() flowcontrolv1beta1.FlowcontrolV1beta1Interface<br>FlowcontrolV1beta2() flowcontrolv1beta2.FlowcontrolV1beta2Interface<br>FlowcontrolV1beta3() flowcontrolv1beta3.FlowcontrolV1beta3Interface<br>NetworkingV1() networkingv1.NetworkingV1Interface<br>NetworkingV1alpha1() networkingv1alpha1.NetworkingV1alpha1Interface<br>NetworkingV1beta1() networkingv1beta1.NetworkingV1beta1Interface<br>NodeV1() nodev1.NodeV1Interface<br>NodeV1alpha1() nodev1alpha1.NodeV1alpha1Interface<br>NodeV1beta1() nodev1beta1.NodeV1beta1Interface<br>PolicyV1() policyv1.PolicyV1Interface<br>PolicyV1beta1() policyv1beta1.PolicyV1beta1Interface<br>RbacV1() rbacv1.RbacV1Interface<br>RbacV1beta1() rbacv1beta1.RbacV1beta1Interface<br>RbacV1alpha1() rbacv1alpha1.RbacV1alpha1Interface<br>ResourceV1alpha2() resourcev1alpha2.ResourceV1alpha2Interface<br>SchedulingV1alpha1() schedulingv1alpha1.SchedulingV1alpha1Interface<br>SchedulingV1beta1() schedulingv1beta1.SchedulingV1beta1Interface<br>SchedulingV1() schedulingv1.SchedulingV1Interface<br>StorageV1beta1() storagev1beta1.StorageV1beta1Interface<br>StorageV1() storagev1.StorageV1Interface<br>StorageV1alpha1() storagev1alpha1.StorageV1alpha1Interface<br>&#125;<br></code></pre></td></tr></table></figure><p>通过接口可以发现都是内置资源。</p><h3 id="DynamicClient"><a href="#DynamicClient" class="headerlink" title="DynamicClient"></a>DynamicClient</h3><p>DynamicClient是一个加强版ClientSet，它除了内置资源外，还可以对CRD资源进行控制。也就是说可以对所有K8S资源对象进行操作。</p><p>之所以DynamicClient能够访问到自定义资源(CRD)，是因为它内部实现了Unstructured，用来处理非结构化数据结构，既无法提前预知的数据结构。</p><p>DynamicClient使用类似于interface{}断言转换的过程，将所有Resouce转换为Unstructured结构类型，所以它并不是类型安全的，在访问CRD自定义资源的时候就需要注意，以免出现像操作指针出现问题导致程序崩溃。</p><h3 id="DiscoveryClient"><a href="#DiscoveryClient" class="headerlink" title="DiscoveryClient"></a>DiscoveryClient</h3><p>用于发现kube-apiserver所支持的资源组、资源版本、资源信息（即Group、Versions、Resources）。主要用于发现K8S API Server所支持的资源组，资源版本，资源信息等等。类似于kubectl api-resource的效果。DiscoveryClient将获取到的资源同步到本地缓存中，每过10分钟和API-Server进行同步更新，因为这些资源变化不大，所以10分钟是一个可以接收的氛围。</p><p><img src="/../images/K8S/image-20240115110928458.png" alt="image-20240115110928458"></p><h2 id="KubeConfig"><a href="#KubeConfig" class="headerlink" title="KubeConfig"></a>KubeConfig</h2><p>kubeconfig是用来管理访问api-server的配置信息，同时也支持访问多kube-apiserver的配置管理，支持不同环境下管理不同kube-apiserver集群配置。kubeconfig中存储了集群、用户、命名空间和身份验证等信息，一个kubeconfig配置文件组成部分如下:</p><ul><li>cluster：表示集群信息，比如说kube-apiserver服务地址以及集群证书信息等等。</li><li>users: 定义访问集群用户的客户端凭据，例如client-certificate、client-key、token及username&#x2F;password等。</li><li>contexts: 定义Kubernetes集群用户信息和命名空间等，用于将请求发送到指定的集群，也就是指定namespace或者全部namespace的访问权限。</li></ul><h2 id="Informer机制"><a href="#Informer机制" class="headerlink" title="Informer机制"></a>Informer机制</h2><p>K8S组件之间使用的是HTTP协议进行通信，通过Informer机制来保证消息在各个组件之间的实时性，可靠性，顺序性等等。</p><p><img src="https://img.ziji-cn-hangzhou.dnsjia.com/2022/02/1424868-20200903225231946-691815526.png" alt="img"></p><h3 id="informer组件"><a href="#informer组件" class="headerlink" title="informer组件"></a>informer组件</h3><ul><li>Reflector</li><li>DeltaFIFO</li><li>Indexer</li></ul><h3 id="Reflector"><a href="#Reflector" class="headerlink" title="Reflector"></a>Reflector</h3><p>Reflector用来监听(watch)资源变化，如果监听到自愿发生变化之后就会触发对应事件，比如Added,Updated,Deleted等等，之后将资源对象存放在本地缓存DeltaFIFO中。</p><p><img src="/../images/K8S/image-20240115112429687.png" alt="image-20240115112429687"></p><h3 id="DeltaFIFO"><a href="#DeltaFIFO" class="headerlink" title="DeltaFIFO"></a>DeltaFIFO</h3><p>DeltaFIFO是一个本地缓存队列，具有基本队列操作方法，比如Add、Update、Delete、List、Pop、Close等等，同时保存资源对象操作类型，比如Added（添加）操作类型、Updated（更新）操作类型、Deleted（删除）操作类型、Sync（同步）操作类型等。</p><h3 id="Indexer"><a href="#Indexer" class="headerlink" title="Indexer"></a>Indexer</h3><p>Indexer也是一个本地缓存，它是一个自带索引功能的本地缓存。它会从DeltaFIFO中去消费，并且它的数据需要和etcd数据保持完全一致，这样就可以减轻API-Server和etcd的负担，client-go只需要从indexer中消费即可。</p><p><img src="/../images/K8S/image-20240115143109998.png" alt="image-20240115143109998"></p><p>从DelataFIFO队列中消费之后，会推送到Workqueue或者其他队列中。</p><h2 id="资源informer"><a href="#资源informer" class="headerlink" title="资源informer"></a>资源informer</h2><p>每个资源都已经实现了informer机制，每一个informer上都实现了infomer和lister方法:</p><figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs go"><span class="hljs-keyword">type</span> PodInformer <span class="hljs-keyword">interface</span> &#123;<br>Informer() cache.SharedIndexInformer<br>Lister() cache.GenericLister<br>&#125;<br></code></pre></td></tr></table></figure><h3 id="Shared-Informer机制"><a href="#Shared-Informer机制" class="headerlink" title="Shared Informer机制"></a>Shared Informer机制</h3><p>通过client-go去使用informer的话，如果是同一个资源就会被实例化多次，并且每个informer都使用一个Reflector会造成太多一样的ListAndWatch，会导致过多的序列化和反序列化操作，导致API-Server负载过高。</p><p>Shared Informer可以使同一类资源Informer共享一个Reflector，这样可以节约很多资源。通过map数据结构实现共享的Informer机制。</p><figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs go"><span class="hljs-keyword">type</span> SharedIndexInformerOptions <span class="hljs-keyword">struct</span> &#123;<br>ResyncPeriod time.Duration<br>Indexers Indexers<br>ObjectDescription <span class="hljs-type">string</span><br>&#125;<br><br><span class="hljs-keyword">type</span> Indexers <span class="hljs-keyword">map</span>[<span class="hljs-type">string</span>]IndexFunc<br></code></pre></td></tr></table></figure><h3 id="ListerWatcher机制"><a href="#ListerWatcher机制" class="headerlink" title="ListerWatcher机制"></a>ListerWatcher机制</h3><p>ListerWatcher主要是Reflector需要, List表示K8S资源需要定期去获取最新状态, 而watch则是对应监控资源变化的, 只要实现了List和Watch方法的对象就可以成为ListerWatcher。Watch通过HTTP协议和API Server建立长连接。</p><h3 id="ThreadSafeMap"><a href="#ThreadSafeMap" class="headerlink" title="ThreadSafeMap"></a>ThreadSafeMap</h3><p>ThreadSafeMap是一个并发安全的存储，具有增、删、改、查操作方法；是一个内存存储，并不会写入到本地磁盘中，每次的增、删、改、查操作都会加锁，以保证数据的一致性。</p><h2 id="WorkQueue"><a href="#WorkQueue" class="headerlink" title="WorkQueue"></a>WorkQueue</h2><p>workQueue和普通队列相比较，实现会复杂一点，主要功能在于标记和去重，并且支持以下特性:</p><ol><li>有序: 按照添加顺序处理元素。</li><li>去重: 相同元素在同一时间不会被重复处理。</li><li>并发性: 多生产者和多消费者。</li><li>标记机制: 标记一个元素是否被处理，也允许元素在处理时重新排队。</li><li>通知机制: ShutDown方法通过信号量通知队列不再接收新的元素，并通知metric goroutine退出。</li><li>延迟: 支持延迟队列，延迟一段时间后再将元素存入队列。</li><li>限速: 元素存入队列时进行速率限制。限制一个元素被重新排队（Reenqueued）的次数。</li><li>Metric: 用于Prometheus监控。</li></ol><p>workQueue支持三种队列，并且提供三种接口，来面对不同使用场景:</p><ol><li>Interface: FIFO队列接口，先进先出队列，并支持去重机制。</li><li>DelayingInterface: 基于Interface接口封装，实现延迟功能。</li><li>RateLimitingInterface: 基于DelayingInterface接口封装，支持元素存入队列时进行速率限制。</li></ol><h3 id="限速算法"><a href="#限速算法" class="headerlink" title="限速算法"></a>限速算法</h3><ul><li>令牌桶算法</li><li>排队指数算法</li><li>计数器算法</li><li>混合模式</li></ul><h2 id="实战-获取event"><a href="#实战-获取event" class="headerlink" title="实战-获取event"></a>实战-获取event</h2><p>K8S event记录中集群上各种事件，这些事件可以很好的帮助运维和开发人员了解到，但是event只保留最近2个小时的，如果集群重启了或者这些事件丢失了，就不好复盘原因。所以我们可以简单地用一个event informer来获取event，然后做一些持久化存储。</p><figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><code class="hljs go"><span class="hljs-keyword">package</span> main<br><br><span class="hljs-keyword">import</span> (<br><span class="hljs-string">&quot;k8s.io/client-go/informers&quot;</span><br><span class="hljs-string">&quot;k8s.io/client-go/kubernetes&quot;</span><br><span class="hljs-string">&quot;k8s.io/client-go/tools/cache&quot;</span><br><span class="hljs-string">&quot;k8s.io/client-go/tools/clientcmd&quot;</span><br><span class="hljs-string">&quot;time&quot;</span><br>)<br><br><span class="hljs-function"><span class="hljs-keyword">func</span> <span class="hljs-title">main</span><span class="hljs-params">()</span></span> &#123;<br>config, err := clientcmd.BuildConfigFromFlags(<span class="hljs-string">&quot;&quot;</span>, <span class="hljs-string">&quot;&quot;</span>)<br><span class="hljs-keyword">if</span> err != <span class="hljs-literal">nil</span> &#123;<br><span class="hljs-built_in">panic</span>(err)<br>&#125;<br>client, err := kubernetes.NewForConfig(config)<br><span class="hljs-keyword">if</span> err != <span class="hljs-literal">nil</span> &#123;<br><span class="hljs-built_in">panic</span>(err)<br>&#125;<br>stopCh := <span class="hljs-built_in">make</span>(<span class="hljs-keyword">chan</span> <span class="hljs-keyword">struct</span>&#123;&#125;)<br><span class="hljs-keyword">defer</span> <span class="hljs-built_in">close</span>(stopCh)<br><br>sharedInformer := informers.NewSharedInformerFactory(client, time.Minute)<br>informer := sharedInformer.Core().V1().Events().Informer()<br>informer.AddEventHandler(cache.ResourceEventHandlerFuncs&#123;<br>AddFunc: <span class="hljs-function"><span class="hljs-keyword">func</span><span class="hljs-params">(obj <span class="hljs-keyword">interface</span>&#123;&#125;)</span></span> &#123;<br><span class="hljs-comment">//事件被创建</span><br>&#125;,<br>UpdateFunc: <span class="hljs-function"><span class="hljs-keyword">func</span><span class="hljs-params">(oldObj, newObj <span class="hljs-keyword">interface</span>&#123;&#125;)</span></span> &#123;<br><span class="hljs-comment">//事件被更新</span><br>&#125;,<br>DeleteFunc: <span class="hljs-function"><span class="hljs-keyword">func</span><span class="hljs-params">(obj <span class="hljs-keyword">interface</span>&#123;&#125;)</span></span> &#123;<br><span class="hljs-comment">//事件被删除</span><br>&#125;,<br>&#125;)<br>informer.Run(stopCh)<br>&#125;<br><br></code></pre></td></tr></table></figure><p>这是一个大概模板，首先我们获取集群的kubeconfig，之后去创建clientSet，informer通过clientSet来和api-Server通信，因为Informer是一个持久运行的goroutine。所以需要使用channel来通知它提前退出。</p><p>NewSharedInformerFactory函数实例化SharedInformer对象，其中两个参数分别是clientSet和resync，resync是用来周期性执行List操作，将所有的资源存放在Informer Store中，如果该参数为0，则禁用resync功能。</p><p>代码中事件对象，AddFunc,UpdateFunc,DeleteFunc都是回调方法，也对应方法名。</p>]]></content>
    
    
    
    <tags>
      
      <tag>kubernetes</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>kubernetes架构设计解析</title>
    <link href="/2024/01/12/kubernetes%E6%9E%B6%E6%9E%84%E8%AE%BE%E8%AE%A1%E8%A7%A3%E6%9E%90/"/>
    <url>/2024/01/12/kubernetes%E6%9E%B6%E6%9E%84%E8%AE%BE%E8%AE%A1%E8%A7%A3%E6%9E%90/</url>
    
    <content type="html"><![CDATA[<h2 id="背景"><a href="#背景" class="headerlink" title="背景"></a>背景</h2><p>之前对kubernetes有过一些简单的了解，但是并没有去深入的去理解每个组件之间是如何工作的，所以正好复习一下。</p><h2 id="概要"><a href="#概要" class="headerlink" title="概要"></a>概要</h2><p>首先看下官方给出来的架构图:</p><p><img src="https://kubernetes.io/images/docs/kubernetes-cluster-architecture.svg" alt="Components of Kubernetes"></p><p>可以看到架构组件分为两类:</p><ul><li>控制平面(control plane)</li><li>节点(node)</li></ul><p>控制平面组件如下:</p><ul><li>kube-api-server</li><li>etcd</li><li>scheduler</li><li>Controller Manager</li><li>cloud-controller-manager</li></ul><p>节点组件如下:</p><ul><li>kubelet</li><li>kube-proxy</li></ul><h2 id="控制平面"><a href="#控制平面" class="headerlink" title="控制平面"></a>控制平面</h2><h3 id="kube-api-server"><a href="#kube-api-server" class="headerlink" title="kube-api-server"></a>kube-api-server</h3><p>api-server主要作用是负责处理客户端请求，包括但不限于kubectl,client-go等等。并且对外提供 RESTful 的接口，也是唯一一个可以和etcd集群通信的组件。</p><p><img src="/..%5Cimages%5CK8S%5Cimage-20240112111009089.png" alt="image-20240112111009089"></p><p>客户端会使用Kubernetes  API来访问集群，API Server在接收请求之后会进行验证:</p><ul><li>身份认证(Authentication): 检查请求的身份信息（例如，证书、Token）。这确保只有经过身份验证的用户才能执行相应的操作。</li><li>授权(Authorization): 确保用户有足够的权限执行请求的操作。</li><li>准入控制(Admission Control): 该阶段可以进行自定义的验证和修改。例如，可以实现资源配额、标签自动填充等功能。</li><li>调用适当的资源对象：API Server 根据请求的 API 路径和操作，调用相应的资源对象处理请求。这可能涉及到对 etcd 中的资源对象进行增、删、改、查等操作。</li><li>存储层（etcd）交互：如果请求涉及对资源对象的增、删、改操作，API Server 会与 etcd 进行交互，将操作写入或读取自 etcd 存储。etcd 用于持久化存储整个集群的配置信息。</li><li>返回结果给客户端: API Server 处理完请求后，将结果返回给客户端。结果中包含了请求的执行状态、资源对象的详细信息等。</li></ul><p>这里用创建pod做一个简单的例子，先暂时忽略其他组件，只关心api server和etcd交互:</p><ol><li>客户端使用 Kubernetes API（例如，通过 kubectl 或自定义的客户端）发送创建 Pod 的 API 请求到 API Server。</li><li>API Server对请求进行校验，包括但不限于身份认证，授权等等。</li><li>API Server根据请求中的路径和操作，调用对应资源处理请求。</li><li>API Server和etcd进行交互，它会构建出创建 Pod 资源对象在 etcd 存储中的键，然后将创建 Pod 的请求写入 etcd 存储。</li><li>etcd 接收到 API Server 的请求后，将创建 Pod 的相关信息持久化到其分布式一致性键值存储中</li><li>最后，API Server 将创建 Pod 的操作结果返回给客户端。结果中可能包含创建成功的信息、Pod 的详细信息等。</li></ol><h3 id="etcd"><a href="#etcd" class="headerlink" title="etcd"></a>etcd</h3><p>etcd存储了整个集群的元数据，一般提到K8S高可用设计，也是指对etcd做高可用，所以可以看出来etcd的重要性。以下是etcd为K8S提供的作用:</p><ol><li>分布式一致性存储: 用来保存和管理 Kubernetes 集群的所有配置数据、状态信息以及元数据。这些数据包括了集群的整体配置、节点信息、服务发现、Pod 状态、网络配置等。etcd 提供了高度一致性和可靠性，确保数据的安全存储和可靠检索。</li><li>集群状态维护: Kubernetes 中的各个组件需要协同工作以保持整个集群的健康状态。etcd 提供了一个共享的数据存储，使得各组件可以轻松地读取和更新集群的状态。这包括了节点的健康状态、服务的注册和发现、Pod 的分配等。</li><li>Configuration Data: etcd 存储了 Kubernetes 集群的所有配置信息，包括 API Server 的配置、节点信息、网络配置、存储卷配置等。这些配置数据可以在运行时动态地更新，而不需要停止整个集群。</li></ol><h3 id="scheduler"><a href="#scheduler" class="headerlink" title="scheduler"></a>scheduler</h3><p>scheduler作用是根据request和limit为Pod调度一个合适的节点，每次只调度一个Pod资源对象，为每一个Pod资源对象寻找合适节点的过程就是一个调度周期，以下是scheduler主要作用:</p><ol><li>节点选择： Scheduler 根据用户定义的调度策略，选择合适的节点来运行新创建的 Pod。这些策略可以包括硬件约束、亲和性（Affinity）和反亲和性（Anti-Affinity）规则、资源需求等。</li><li>负载均衡： Scheduler 努力确保各个节点的资源负载相对均衡，以充分利用整个集群的计算资源。通过智能地分配 Pod，Scheduler 可以避免资源过度集中在某些节点上，提高整个集群的利用率。</li><li>调度决策： Scheduler 基于当前集群状态和用户定义的策略，做出调度决策。这可能涉及节点资源的可用性、Pod 的优先级、亲和性规则等多个因素的权衡。</li><li>动态调度： Kubernetes 的 Scheduler 不仅在 Pod 创建时进行调度，还会监控集群状态的变化，动态地进行重新调度。例如，当节点故障或新节点加入集群时，Scheduler 可以重新调整现有 Pod 的分布，确保它们仍然满足调度策略。</li><li>可插拔性： Kubernetes 的 Scheduler 是可插拔的，用户可以根据自己的需求实现自定义的调度器，并将其集成到 Kubernetes 中。这样可以根据特定的业务场景或策略来定制调度行为。</li></ol><h3 id="Controller-Manager"><a href="#Controller-Manager" class="headerlink" title="Controller Manager"></a>Controller Manager</h3><p>Controller Manager管理K8S集群中节点，Pod副本，服务，端点(Endpoint)，命名空间等等。举个稍微复杂的例子，假设K8S是一个房子，房子里面会有各种各样的设备，比如空调，热水器，温度调节器等等。为了让房间内更加舒服。你把温度调节器设置在30度，同时又想洗个热水澡，去把热水器调到50度。这些都是你的期望状态。之后这些设备开始工作，它会确保这些设备达到这些期望状态。这些设备就是控制器(Controller)，而Controller Manager就是统一管理这些控制器的。</p><p><img src="D:\code\blog\source\images\K8S\image-20240112131548620.png" alt="image-20240112131548620"></p><h3 id="cloud-controller-manager"><a href="#cloud-controller-manager" class="headerlink" title="cloud-controller-manager"></a>cloud-controller-manager</h3><p>这一块和Controller Manager差不多，主要是云厂商关心的比较多，这里就不过多介绍了。</p><h2 id="Node"><a href="#Node" class="headerlink" title="Node"></a>Node</h2><h3 id="kubelet"><a href="#kubelet" class="headerlink" title="kubelet"></a>kubelet</h3><p>kubelet运行在K8S每个节点上，用来接收，处理，上报kube-apiserver组件下发的任务，kubelet进程启动的时候会向kube-apiserver注册节点自身信息。</p><p>主要负责所在节点上的Pod资源对象管理，比如说Pod资源对象的创建、修改、监控、删除、驱逐及Pod生命周期管理等。并且周期性的上报节点信息给kube-apiserver组件。kubelet也会对所在节点的镜像和容器做清理工作，保证节点上的镜像不会占满磁盘空间、删除的容器释放相关资源。</p><p>除了上报信息之外，它还提供了三种接口:</p><ol><li>Container Runtime Interface：简称CRI（容器运行时接口）：提供容器运行时通用插件接口服务。</li><li>Container Network Interface: 简称CNI(容器网络接口): 提供网络通用插件接口服务。</li><li>Container Storage Interface: 简称CSI(容器存储接口): 提供存储通用插件接口服务。</li></ol><p>通过以上信息可以大概知道，kubelet主要作用是收集上报信息和通过三种接口去创建对应资源。</p><h3 id="kube-proxy"><a href="#kube-proxy" class="headerlink" title="kube-proxy"></a>kube-proxy</h3><p>kube-proxy作为节点上的网络代理，和kubelet一样，运行在每个节点上。它监控kube-apiserver的服务和端点资源变化，并通过iptables&#x2F;ipvs等配置负载均衡器，为一组Pod提供统一的TCP&#x2F;UDP流量转发和负载均衡功能。</p><p>kube-proxy是参与管理Pod-to-Service和External-to-Service网络的最重要的节点组件之一，以下是它的主要功能和作用:</p><ol><li><strong>服务代理：</strong> <code>kube-proxy</code> 负责监听 Kubernetes 集群中的服务创建和删除事件。当有新的服务创建时，<code>kube-proxy</code> 会为该服务创建对应的代理规则。这些代理规则可以将服务的访问请求转发到正确的 Pod 上。</li><li><strong>负载均衡：</strong> 对于 Service 类型为 <code>LoadBalancer</code> 或 <code>NodePort</code> 的服务，<code>kube-proxy</code> 在每个节点上配置相应的规则，以实现负载均衡。这样，外部流量可以通过任何节点访问到服务，并且请求会被均匀分布到后端的 Pod 上。</li><li><strong>服务发现：</strong> <code>kube-proxy</code> 通过与 Kubernetes API 交互，获取服务的信息，包括服务的 IP 地址和端口号，以及后端 Pod 的 IP 地址和端口号。这样，它能够动态地更新代理规则，确保服务发现的准确性。</li><li><strong>网络代理：</strong> 对于 Service 类型为 <code>ClusterIP</code> 的服务，<code>kube-proxy</code> 通过 iptables 或 IPVS 等技术，创建一组虚拟 IP 和端口规则，将流量代理到后端的 Pod。这样，即使 Pod 的 IP 地址发生变化，服务的虚拟 IP 不会改变，保证了服务的稳定性</li></ol><p>总的来说，K8S Service是通过kube-proxy来实现的，它又是使用了iptables ,IPVS等等技术来实现Service。</p><h2 id="Pod创建-查看组件之间调用"><a href="#Pod创建-查看组件之间调用" class="headerlink" title="Pod创建-查看组件之间调用"></a>Pod创建-查看组件之间调用</h2><p>这里使用Pod创建来查看组件之间是如何配合的，以及它的一个调用链路。</p>]]></content>
    
    
    
    <tags>
      
      <tag>kubernetes</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>从零实现一个operator(1)-kubebuilder实现redis operator</title>
    <link href="/2024/01/11/%E4%BB%8E%E9%9B%B6%E5%AE%9E%E7%8E%B0%E4%B8%80%E4%B8%AAoperator-1-kubebuilder%E5%AE%9E%E7%8E%B0redis-operator/"/>
    <url>/2024/01/11/%E4%BB%8E%E9%9B%B6%E5%AE%9E%E7%8E%B0%E4%B8%80%E4%B8%AAoperator-1-kubebuilder%E5%AE%9E%E7%8E%B0redis-operator/</url>
    
    <content type="html"><![CDATA[<h2 id="operator实现步骤"><a href="#operator实现步骤" class="headerlink" title="operator实现步骤"></a>operator实现步骤</h2><p>正式开始使用kubebuilder来实现operator，关于operator原理可以看之前文章介绍，根据官网教程中的步骤来初始化一个operator项目:</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">kubebuilder init --domain my.domain --repo my.domain/guestbook <span class="hljs-comment">#修改这里的my.domain和my.domain/guestbook</span><br></code></pre></td></tr></table></figure><p>之后kubebuidler会帮助我们生成相关代码:</p><p><img src="/../images/K8S/image-20240115173047116.png" alt="image-20240115173047116"></p><ul><li>PROJECT: 关于项目的一些元数据，比如domain、projectName、repo等信息。</li><li>config: 一些关于RBAC权限的yaml文件，以及Prometheus监控发现的相关yaml文件，还有控制器部署的yaml文件。</li><li>Dockerfile: 整个代码完成之后，打包成镜像使用。</li><li>Makefile: 整个程序的编译构建，镜像推送、部署、卸载等操作。</li></ul><h3 id="定义CRD"><a href="#定义CRD" class="headerlink" title="定义CRD"></a>定义CRD</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">kubebuilder create api --group redis  --version v1 --kind Standalone<br></code></pre></td></tr></table></figure><p>通过以上命令创建CRD，之后可以看到相关yaml:</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><code class="hljs yaml"><span class="hljs-attr">apiVersion:</span> <span class="hljs-string">redis.cola.redis/v1</span><br><span class="hljs-string">z</span><br><span class="hljs-attr">metadata:</span><br>  <span class="hljs-attr">labels:</span><br>    <span class="hljs-attr">app.kubernetes.io/name:</span> <span class="hljs-string">standalone</span><br>    <span class="hljs-attr">app.kubernetes.io/instance:</span> <span class="hljs-string">standalone-sample</span><br>    <span class="hljs-attr">app.kubernetes.io/part-of:</span> <span class="hljs-string">kubebuilder</span><br>    <span class="hljs-attr">app.kubernetes.io/managed-by:</span> <span class="hljs-string">kustomize</span><br>    <span class="hljs-attr">app.kubernetes.io/created-by:</span> <span class="hljs-string">kubebuilder</span><br>  <span class="hljs-attr">name:</span> <span class="hljs-string">standalone-sample</span><br><span class="hljs-attr">spec:</span><br>  <span class="hljs-comment"># TODO(user): Add fields here</span><br><br></code></pre></td></tr></table></figure><p>yaml中字段正好对应着命令中的各种信息，api&#x2F;v1&#x2F;standalone_types.go中有关于CRD的代码实现:</p><figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><code class="hljs go"><span class="hljs-comment">// StandaloneSpec defines the desired state of Standalone</span><br><span class="hljs-keyword">type</span> StandaloneSpec <span class="hljs-keyword">struct</span> &#123;<br><span class="hljs-comment">// INSERT ADDITIONAL SPEC FIELDS - desired state of cluster</span><br><span class="hljs-comment">// Important: Run &quot;make&quot; to regenerate code after modifying this file</span><br><br><span class="hljs-comment">// Foo is an example field of Standalone. Edit standalone_types.go to remove/update</span><br>Foo <span class="hljs-type">string</span> <span class="hljs-string">`json:&quot;foo,omitempty&quot;`</span><br>&#125;<br><br><span class="hljs-comment">// StandaloneStatus defines the observed state of Standalone</span><br><span class="hljs-keyword">type</span> StandaloneStatus <span class="hljs-keyword">struct</span> &#123;<br><span class="hljs-comment">// INSERT ADDITIONAL STATUS FIELD - define observed state of cluster</span><br><span class="hljs-comment">// Important: Run &quot;make&quot; to regenerate code after modifying this file</span><br>&#125;<br><br><span class="hljs-comment">//+kubebuilder:object:root=true</span><br><span class="hljs-comment">//+kubebuilder:subresource:status</span><br><br><span class="hljs-comment">// Standalone is the Schema for the standalones API</span><br><span class="hljs-keyword">type</span> Standalone <span class="hljs-keyword">struct</span> &#123;<br>metav1.TypeMeta   <span class="hljs-string">`json:&quot;,inline&quot;`</span><br>metav1.ObjectMeta <span class="hljs-string">`json:&quot;metadata,omitempty&quot;`</span><br><br>Spec   StandaloneSpec   <span class="hljs-string">`json:&quot;spec,omitempty&quot;`</span><br>Status StandaloneStatus <span class="hljs-string">`json:&quot;status,omitempty&quot;`</span><br>&#125;<br><br><span class="hljs-comment">//+kubebuilder:object:root=true</span><br><br><span class="hljs-comment">// StandaloneList contains a list of Standalone</span><br><span class="hljs-keyword">type</span> StandaloneList <span class="hljs-keyword">struct</span> &#123;<br>metav1.TypeMeta <span class="hljs-string">`json:&quot;,inline&quot;`</span><br>metav1.ListMeta <span class="hljs-string">`json:&quot;metadata,omitempty&quot;`</span><br>Items           []Standalone <span class="hljs-string">`json:&quot;items&quot;`</span><br>&#125;<br><br><span class="hljs-function"><span class="hljs-keyword">func</span> <span class="hljs-title">init</span><span class="hljs-params">()</span></span> &#123;<br>SchemeBuilder.Register(&amp;Standalone&#123;&#125;, &amp;StandaloneList&#123;&#125;)<br>&#125;<br></code></pre></td></tr></table></figure><h3 id="部署CRD"><a href="#部署CRD" class="headerlink" title="部署CRD"></a>部署CRD</h3><p>当CRD代码实现之后，可以通过make manifests来生成ClusterRole和CustomResourceDefinition配置。</p><ul><li>config&#x2F;crd&#x2F;bases&#x2F;crdFile.yaml</li><li>config&#x2F;rbac&#x2F;role.yaml</li></ul><p>相关CRD文件</p><p><img src="/../images/K8S/image-20240115174420263.png" alt="image-20240115174420263"></p><p>之后通过make install来部署这个CRD，之后通过api-resource或者kubectl get crd都能够看到这个CRD。</p><h3 id="控制器-Controller-实现"><a href="#控制器-Controller-实现" class="headerlink" title="控制器(Controller)实现"></a>控制器(Controller)实现</h3><p>虽然CR对应的yaml可以被创建，但是相关的pod是不会运行的，因为没有控制器，等控制器实现之后，整个operator就算是完成了。</p><h3 id="控制器部署"><a href="#控制器部署" class="headerlink" title="控制器部署"></a>控制器部署</h3><h2 id="demo实现"><a href="#demo实现" class="headerlink" title="demo实现"></a>demo实现</h2><p>根据之前步骤，可以明白一个operator是怎么开发出来的，这里参考ot-redis-operator中的Standalone，来开发一个单例redis operator。先来看ot-redis-operator的例子:</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><code class="hljs yaml"><span class="hljs-meta">---</span><br><span class="hljs-attr">apiVersion:</span> <span class="hljs-string">redis.redis.opstreelabs.in/v1beta1</span><br><span class="hljs-attr">kind:</span> <span class="hljs-string">Redis</span><br><span class="hljs-attr">metadata:</span><br>  <span class="hljs-attr">name:</span> <span class="hljs-string">redis-standalone</span><br><span class="hljs-attr">spec:</span><br>  <span class="hljs-attr">kubernetesConfig:</span><br>    <span class="hljs-attr">image:</span> <span class="hljs-string">quay.io/opstree/redis:v7.0.5</span><br>    <span class="hljs-attr">imagePullPolicy:</span> <span class="hljs-string">IfNotPresent</span><br>  <span class="hljs-attr">storage:</span><br>    <span class="hljs-attr">volumeClaimTemplate:</span><br>      <span class="hljs-attr">spec:</span><br>        <span class="hljs-comment"># storageClassName: standard</span><br>        <span class="hljs-attr">accessModes:</span> [<span class="hljs-string">&quot;ReadWriteOnce&quot;</span>]<br>        <span class="hljs-attr">resources:</span><br>          <span class="hljs-attr">requests:</span><br>            <span class="hljs-attr">storage:</span> <span class="hljs-string">1Gi</span><br>  <span class="hljs-attr">securityContext:</span><br>    <span class="hljs-attr">runAsUser:</span> <span class="hljs-number">1000</span><br>    <span class="hljs-attr">fsGroup:</span> <span class="hljs-number">1000</span><br><br></code></pre></td></tr></table></figure><p>这一块对应了Sepc代码:</p><figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><code class="hljs go"><span class="hljs-comment">// RedisSpec defines the desired state of Redis</span><br><span class="hljs-keyword">type</span> RedisSpec <span class="hljs-keyword">struct</span> &#123;<br>KubernetesConfig   KubernetesConfig           <span class="hljs-string">`json:&quot;kubernetesConfig&quot;`</span><br>RedisExporter      *RedisExporter             <span class="hljs-string">`json:&quot;redisExporter,omitempty&quot;`</span><br>RedisConfig        *RedisConfig               <span class="hljs-string">`json:&quot;redisConfig,omitempty&quot;`</span><br>Storage            *Storage                   <span class="hljs-string">`json:&quot;storage,omitempty&quot;`</span><br>NodeSelector       <span class="hljs-keyword">map</span>[<span class="hljs-type">string</span>]<span class="hljs-type">string</span>          <span class="hljs-string">`json:&quot;nodeSelector,omitempty&quot;`</span><br>PodSecurityContext *corev1.PodSecurityContext <span class="hljs-string">`json:&quot;podSecurityContext,omitempty&quot;`</span><br>SecurityContext    *corev1.SecurityContext    <span class="hljs-string">`json:&quot;securityContext,omitempty&quot;`</span><br>PriorityClassName  <span class="hljs-type">string</span>                     <span class="hljs-string">`json:&quot;priorityClassName,omitempty&quot;`</span><br>Affinity           *corev1.Affinity           <span class="hljs-string">`json:&quot;affinity,omitempty&quot;`</span><br>Tolerations        *[]corev1.Toleration       <span class="hljs-string">`json:&quot;tolerations,omitempty&quot;`</span><br>TLS                *TLSConfig                 <span class="hljs-string">`json:&quot;TLS,omitempty&quot;`</span><br>ACL                *ACLConfig                 <span class="hljs-string">`json:&quot;acl,omitempty&quot;`</span><br><span class="hljs-comment">// +kubebuilder:default:=&#123;initialDelaySeconds: 1, timeoutSeconds: 1, periodSeconds: 10, successThreshold: 1, failureThreshold:3&#125;</span><br>ReadinessProbe *Probe <span class="hljs-string">`json:&quot;readinessProbe,omitempty&quot; protobuf:&quot;bytes,11,opt,name=readinessProbe&quot;`</span><br><span class="hljs-comment">// +kubebuilder:default:=&#123;initialDelaySeconds: 1, timeoutSeconds: 1, periodSeconds: 10, successThreshold: 1, failureThreshold:3&#125;</span><br>LivenessProbe                 *Probe           <span class="hljs-string">`json:&quot;livenessProbe,omitempty&quot; protobuf:&quot;bytes,11,opt,name=livenessProbe&quot;`</span><br>InitContainer                 *InitContainer   <span class="hljs-string">`json:&quot;initContainer,omitempty&quot;`</span><br>Sidecars                      *[]Sidecar       <span class="hljs-string">`json:&quot;sidecars,omitempty&quot;`</span><br>ServiceAccountName            *<span class="hljs-type">string</span>          <span class="hljs-string">`json:&quot;serviceAccountName,omitempty&quot;`</span><br>TerminationGracePeriodSeconds *<span class="hljs-type">int64</span>           <span class="hljs-string">`json:&quot;terminationGracePeriodSeconds,omitempty&quot; protobuf:&quot;varint,4,opt,name=terminationGracePeriodSeconds&quot;`</span><br>EnvVars                       *[]corev1.EnvVar <span class="hljs-string">`json:&quot;env,omitempty&quot;`</span><br>&#125;<br></code></pre></td></tr></table></figure><p>这里可以从使用者角度出发，你可以想象一下，如果你需要一个redis实例，你希望填写哪些参数就可以创建出一个redis？这里贴一下我的spec，后续有需要可以在修改，也可以参考redis config中哪些参数是需要的:</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><code class="hljs yaml"><span class="hljs-attr">apiVersion:</span> <span class="hljs-string">redis.cola.redis/v1</span><br><span class="hljs-attr">kind:</span> <span class="hljs-string">Standalone</span><br><span class="hljs-attr">metadata:</span><br>  <span class="hljs-attr">labels:</span><br>    <span class="hljs-attr">app.kubernetes.io/name:</span> <span class="hljs-string">standalone</span><br>    <span class="hljs-attr">app.kubernetes.io/instance:</span> <span class="hljs-string">standalone-sample</span><br>    <span class="hljs-attr">app.kubernetes.io/part-of:</span> <span class="hljs-string">kubebuilder</span><br>    <span class="hljs-attr">app.kubernetes.io/managed-by:</span> <span class="hljs-string">kustomize</span><br>    <span class="hljs-attr">app.kubernetes.io/created-by:</span> <span class="hljs-string">kubebuilder</span><br>  <span class="hljs-attr">name:</span> <span class="hljs-string">standalone-sample</span><br><span class="hljs-attr">spec:</span><br>  <span class="hljs-comment"># TODO(user): Add fields here</span><br>  <span class="hljs-comment">#image,imagePullPolicy,resources,</span><br> <span class="hljs-attr">redisConfigFile:</span> <span class="hljs-comment">#user.monitoring,storage</span><br> <span class="hljs-attr">port:</span><br> <span class="hljs-attr">version:</span><br> <span class="hljs-attr">resourceReqs:</span><br></code></pre></td></tr></table></figure><h2 id="参考链接"><a href="#参考链接" class="headerlink" title="参考链接"></a>参考链接</h2><ul><li><a href="https://book.kubebuilder.io/quick-start">https://book.kubebuilder.io/quick-start</a></li><li><a href="https://ot-redis-operator.netlify.app/docs/">https://ot-redis-operator.netlify.app/docs/</a></li><li>Kubernetes Operator开发进阶</li></ul>]]></content>
    
    
    
    <tags>
      
      <tag>kubernetes</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>从零实现一个operator(0)-前期准备</title>
    <link href="/2024/01/11/%E4%BB%8E%E9%9B%B6%E5%AE%9E%E7%8E%B0%E4%B8%80%E4%B8%AAoperator-0-%E5%89%8D%E6%9C%9F%E5%87%86%E5%A4%87/"/>
    <url>/2024/01/11/%E4%BB%8E%E9%9B%B6%E5%AE%9E%E7%8E%B0%E4%B8%80%E4%B8%AAoperator-0-%E5%89%8D%E6%9C%9F%E5%87%86%E5%A4%87/</url>
    
    <content type="html"><![CDATA[<h2 id="operator基本概念"><a href="#operator基本概念" class="headerlink" title="operator基本概念"></a>operator基本概念</h2><p>在官方给出的解释中，operator是K8S用来对定制资源进行管理的组件，它遵守控制器理念。控制器理念是指个非终止回路，用于调节系统状态。</p><p>看到以上解释，肯定还是不明白什么是operator，operator这个词是从机器人技术和自动化领域借用过来的，举个例子来说下，假设你有一个非常高级的空调，可以调节室内温度，你希望室内温度在26度左右，这是你的期望值。而控制器换句话说operator，可以保证当前温度就是你的期望状态。</p><p>回到K8S本身，K8S本身就有很多这种控制器，举个例子，deployment就是一个典型的控制器:</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><code class="hljs yaml"><span class="hljs-attr">apiVersion:</span> <span class="hljs-string">apps/v1</span><br><span class="hljs-attr">kind:</span> <span class="hljs-string">Deployment</span><br><span class="hljs-attr">metadata:</span><br>  <span class="hljs-attr">name:</span> <span class="hljs-string">nginx-deployment</span><br>  <span class="hljs-attr">labels:</span><br>    <span class="hljs-attr">app:</span> <span class="hljs-string">nginx</span><br><span class="hljs-attr">spec:</span><br>  <span class="hljs-attr">replicas:</span> <span class="hljs-number">3</span><br>  <span class="hljs-attr">selector:</span><br>    <span class="hljs-attr">matchLabels:</span><br>      <span class="hljs-attr">app:</span> <span class="hljs-string">nginx</span><br>  <span class="hljs-attr">template:</span><br>    <span class="hljs-attr">metadata:</span><br>      <span class="hljs-attr">labels:</span><br>        <span class="hljs-attr">app:</span> <span class="hljs-string">nginx</span><br>    <span class="hljs-attr">spec:</span><br>      <span class="hljs-attr">containers:</span><br>      <span class="hljs-bullet">-</span> <span class="hljs-attr">name:</span> <span class="hljs-string">nginx</span><br>        <span class="hljs-attr">image:</span> <span class="hljs-string">nginx:1.14.2</span><br>        <span class="hljs-attr">ports:</span><br>        <span class="hljs-bullet">-</span> <span class="hljs-attr">containerPort:</span> <span class="hljs-number">80</span><br></code></pre></td></tr></table></figure><p>这里我们只关心replicas，这是我的一个期望状态，我希望有3个Pod被创建同时在运行。Deployment会创建一个 replicasets， replicasets会帮助我们创建3个Pod。这里先不关心细节。也就说控制器会按照期望值去创建Pod数量。</p><p>通过前面的铺垫可以清楚的知道operator是什么了，可以简单粗暴地认为operator&#x3D;resource+controller，resource就是deployment,replicasets,pod等等。controller就是deployment controller。</p><p>K8S中虽然有各种各样的controller，但是肯定也会有K8S不支持的，去修改源码难度太大，并且会造成高耦合。所以K8S提供了CRD(CustomResourceDefinitions)。有了这个我们就可以实现自己的operator了。</p><h2 id="demo设计"><a href="#demo设计" class="headerlink" title="demo设计"></a>demo设计</h2><p>这里我们设计一个redis operator，这一块像ucloud和ot-redis-operator都是有现成的，一般情况下，可以根据这些去做二次开发，但是因为是从零开始，所以这里就自己造轮子。redis有单机模式(Standalone)，集群模式(Cluster)，哨兵模式(Sentinel)，主从模式(Replication)；为了方便，先实现单机模式。</p><h2 id="开发工具"><a href="#开发工具" class="headerlink" title="开发工具"></a>开发工具</h2><ul><li>手动实现</li><li>operator SDK</li><li>kubebuilder</li></ul><h3 id="手动实现"><a href="#手动实现" class="headerlink" title="手动实现"></a>手动实现</h3><p>手动实现的好处是可以帮助我们更加深入的了解Operator工作原理，但是会增加开发和维护成本。并且会有大量的重复。</p><h3 id="operator-SDK"><a href="#operator-SDK" class="headerlink" title="operator SDK"></a>operator SDK</h3><p>Operator SDK 是一个由 CoreOS（现在是 Red Hat 的一部分）提供的开发框架，用于简化 Kubernetes Operator 的创建过程。它主要关注于快速开发、测试和迭代 Operator。Operator SDK 提供了三种主要的 Operator 类型：Go、Ansible 和 Helm。其中，Go 类型的 Operator 是 SDK 的核心。</p><p><strong>特点：</strong></p><ol><li><strong>代码生成：</strong> 提供了命令行工具，可以轻松生成 Operator 所需的代码模板，包括 Custom Resource Definitions（CRD）和 Controller 的基本结构。</li><li><strong>快速开发：</strong> 提供了一系列的库和工具，帮助开发者迅速搭建 Operator，减少重复性工作。</li><li><strong>内置测试框架：</strong> 集成了测试框架，方便进行单元测试和集成测试。</li><li><strong>支持多版本：</strong> 支持 Go 版本的 Operator，可以用于创建不同版本的 CRD。</li></ol><p><strong>优势：</strong></p><ol><li><strong>快速开发：</strong> 提供了一些开发 Operator 所需的基础设施，可以加速开发过程。</li><li><strong>代码生成：</strong> 自动生成大部分模板代码，减少了重复性劳动。</li><li><strong>集成测试：</strong> 内置了测试框架，支持容易进行集成测试。</li></ol><p><strong>劣势：</strong></p><ol><li><strong>定制性差：</strong> 相对于手动实现，可能定制性较差，不够灵活。</li><li><strong>学习成本：</strong> 学习使用 SDK 的一些概念和工具可能需要一些时间</li></ol><h3 id="kubebuilder"><a href="#kubebuilder" class="headerlink" title="kubebuilder"></a>kubebuilder</h3><p>Kubebuilder 是另一个强大的 Operator 开发框架，构建在 controller-runtime 和 controller-tools 之上，提供了更高级的特性。</p><p><strong>特点：</strong></p><ol><li><strong>强大的框架：</strong> Kubebuilder 提供了一个强大的框架，使开发者能够创建和管理 Kubernetes Operator，同时提供高级的特性。</li><li><strong>代码生成：</strong> 类似于 Operator SDK，Kubebuilder 提供了代码生成工具，帮助生成 CRD、Controller 和一些基础代码。</li><li><strong>支持多版本：</strong> 支持定义多个 API 版本，方便进行迭代和演进。</li><li><strong>控制器运行时：</strong> 使用 controller-runtime 库，提供了用于编写控制器的核心运行时框架。</li></ol><p><strong>优势：</strong></p><ol><li><strong>强大的框架：</strong> Kubebuilder 是一个基于 controller-runtime 和 controller-tools 的强大框架，提供了一些高级特性。</li><li><strong>代码生成：</strong> 类似于 Operator SDK，提供了代码生成工具，减少了开发工作量。</li><li><strong>支持多种 API 版本：</strong> 支持定义多个 API 版本，方便进行迭代和演进。</li></ol><p><strong>劣势：</strong></p><ol><li><strong>学习成本：</strong> 学习 Kubebuilder 框架可能相对较复杂，需要一些时间来掌握。</li></ol><p>Kubebuilder 适用于对 Operator 需求较为复杂、需要高级特性的项目。它提供了强大的框架和工具，支持构建复杂的 Operator。</p><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>这里我们先使用kubebuilder来实现redis operator，之后再根据kubebuilder去自己实现一套控制器相关代码。</p><h2 id="参考链接"><a href="#参考链接" class="headerlink" title="参考链接"></a>参考链接</h2><ul><li><a href="https://kubernetes.io/zh-cn/docs/concepts/extend-kubernetes/operator/">https://kubernetes.io/zh-cn/docs/concepts/extend-kubernetes/operator/</a></li><li><a href="https://kubernetes.io/zh-cn/docs/concepts/architecture/controller/">https://kubernetes.io/zh-cn/docs/concepts/architecture/controller/</a></li><li><a href="https://kubernetes.io/docs/tasks/extend-kubernetes/custom-resources/custom-resource-definitions/">https://kubernetes.io/docs/tasks/extend-kubernetes/custom-resources/custom-resource-definitions/</a></li><li><a href="https://ot-redis-operator.netlify.app/docs/overview/">https://ot-redis-operator.netlify.app/docs/overview/</a></li></ul>]]></content>
    
    
    
    <tags>
      
      <tag>kubernetes</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>连接同一个局域网下的HomeLab</title>
    <link href="/2024/01/06/%E8%BF%9E%E6%8E%A5%E5%90%8C%E4%B8%80%E4%B8%AA%E5%B1%80%E5%9F%9F%E7%BD%91%E4%B8%8B%E7%9A%84HomeLab/"/>
    <url>/2024/01/06/%E8%BF%9E%E6%8E%A5%E5%90%8C%E4%B8%80%E4%B8%AA%E5%B1%80%E5%9F%9F%E7%BD%91%E4%B8%8B%E7%9A%84HomeLab/</url>
    
    <content type="html"><![CDATA[<h2 id="背景"><a href="#背景" class="headerlink" title="背景"></a>背景</h2><p>因为之前搬家，搭建好的HomeLab使用的是静态IP，所以当搬家完之后，就需要重新配置过。</p><h2 id="局域网内ping测试"><a href="#局域网内ping测试" class="headerlink" title="局域网内ping测试"></a>局域网内ping测试</h2><p>之前远程连接用的是Windows自带的，相关配置不需要修改，只需要修改下IP即可。但是想要使用HomeLab里面的各种服务，比如说虚拟机搭建的K8S集群，或者说访问Web服务，这里还不够。</p><p><img src="/../images/homelab/image-20240106094829307.png" alt="image-20240106094829307"></p><p>当我把HomeLab防火墙关闭掉的时候，就可以ping通:</p><p><img src="/../images/homelab/image-20240106100630877.png" alt="image-20240106100630877"></p><p>很明显这样做风险太大，可以通过出入站规则来控制。</p><h2 id="访问HomeLab服务"><a href="#访问HomeLab服务" class="headerlink" title="访问HomeLab服务"></a>访问HomeLab服务</h2><p>顾名思义，出站规则就是HomeLab将内部流量转发出来，而入站规则是将流量发送到HomeLab中:</p><p><img src="/../images/homelab/image-20240106101650232.png" alt="image-20240106101837654"></p><p>大致如上图所示，假设需要访问HomeLab上服务就添加对应规则即可。但是访问VMware虚拟机会比较麻烦，因为VMware虚拟机中使用了NAT服务，IP又被转换了一次。为了能够访问到VMware虚拟机，需要在进行一次修改。</p><h3 id="Vmware虚拟机服务访问"><a href="#Vmware虚拟机服务访问" class="headerlink" title="Vmware虚拟机服务访问"></a>Vmware虚拟机服务访问</h3><p>这里需要把VMware的Vmnet8网卡进行编辑，让它充当一个NAT角色:</p><p><img src="/../images/homelab/image-20240106110929962.png" alt="image-20240106110929962"></p><p>在控制面板&gt;&gt;网络和 Internet&gt;&gt;网络连接中选择VMnet8网卡，然后修改它的ipv4属性:</p><p><img src="C:\Users\wuliang\AppData\Roaming\Typora\typora-user-images\image-20240106112216306.png" alt="image-20240106112216306"></p><p><img src="C:\Users\wuliang\AppData\Roaming\Typora\typora-user-images\image-20240106112231398.png" alt="image-20240106112231398"></p><p>IP地址和默认网关应该都在同一个网段里面，这一段和虚拟机地址差不多。</p><p><img src="/../images/homelab/image-20240108195652154.png" alt="image-20240108195652154"></p><p>设置完成之后，需要回到虚拟机中修改下网卡配置:</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs bash">vim /etc/sysconfig/network-scripts/ifcfg-xxxx <span class="hljs-comment">#这里是你的网卡名称</span><br><br></code></pre></td></tr></table></figure><p><img src="/../images/homelab/image-20240108195849802.png" alt="image-20240108195849802"></p><p>设置前面的网关IP</p><p><img src="/../images/homelab/image-20240106174326377.png" alt="image-20240106174326377"></p><p>之后在这里对Vmnet8网卡编辑:</p><ul><li>点击 VMware 的「编辑(e)」，选择「虚拟机网络编辑器」</li><li>选择 VMnet8 ，在点击「NAT 设置」</li></ul><p><img src="/../images/homelab/image-20240106111012702.png" alt="image-20240106111012702"></p><p><img src="/../images/homelab/image-20240106111122057.png" alt="image-20240106111122057"></p><p>最后配置入站和出站规则:</p><p><img src="/../images/homelab/image-20240106180154166.png" alt="image-20240106180154166"></p><p><img src="/../images/homelab/image-20240106180211844.png" alt="image-20240106180211844"></p><p>这个时候同一个局域网下的机器只需要通过ssh连接HomeLabIP+port，我这里是22，就可以直接使用了</p><p><img src="/../images/homelab/image-20240108200109422.png" alt="image-20240108200109422"></p><h2 id="参考连接"><a href="#参考连接" class="headerlink" title="参考连接"></a>参考连接</h2><ul><li><a href="https://zhuanlan.zhihu.com/p/130984945">https://zhuanlan.zhihu.com/p/130984945</a></li><li><a href="https://mengxiaoxing.top/2021/09/18/vmnat%E7%BD%91%E7%BB%9C%E6%A8%A1%E5%BC%8F%E5%B1%80%E5%9F%9F%E7%BD%91%E8%AE%BF%E9%97%AE%E8%99%9A%E6%8B%9F%E6%9C%BA/">https://mengxiaoxing.top/2021/09/18/vmnat%E7%BD%91%E7%BB%9C%E6%A8%A1%E5%BC%8F%E5%B1%80%E5%9F%9F%E7%BD%91%E8%AE%BF%E9%97%AE%E8%99%9A%E6%8B%9F%E6%9C%BA/</a></li></ul>]]></content>
    
    
    
    <tags>
      
      <tag>homeLab</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>异常检测论文-FRAUDAR</title>
    <link href="/2024/01/03/%E5%BC%82%E5%B8%B8%E6%A3%80%E6%B5%8B%E8%AE%BA%E6%96%87-FRAUDAR/"/>
    <url>/2024/01/03/%E5%BC%82%E5%B8%B8%E6%A3%80%E6%B5%8B%E8%AE%BA%E6%96%87-FRAUDAR/</url>
    
    <content type="html"><![CDATA[<h2 id="背景"><a href="#背景" class="headerlink" title="背景"></a>背景</h2><h2 id="解决的问题"><a href="#解决的问题" class="headerlink" title="解决的问题"></a>解决的问题</h2><h2 id="算法应用场景"><a href="#算法应用场景" class="headerlink" title="算法应用场景"></a>算法应用场景</h2><h2 id="伪装"><a href="#伪装" class="headerlink" title="伪装"></a>伪装</h2><ul><li>随机伪装</li><li>有偏伪装</li><li>劫持账户</li></ul><h2 id="抗伪装"><a href="#抗伪装" class="headerlink" title="抗伪装"></a>抗伪装</h2><h2 id="异常指标定义"><a href="#异常指标定义" class="headerlink" title="异常指标定义"></a>异常指标定义</h2><ul><li>点异常</li><li>边异常</li></ul><h3 id="公理"><a href="#公理" class="headerlink" title="公理"></a>公理</h3><h2 id="算法流程"><a href="#算法流程" class="headerlink" title="算法流程"></a>算法流程</h2><h2 id="思考"><a href="#思考" class="headerlink" title="思考"></a>思考</h2><ul><li>最开始的时候，如何定义或者判断是异常值？</li><li></li></ul>]]></content>
    
    
    
    <tags>
      
      <tag>AIOPS</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>异常检测(一)-导论</title>
    <link href="/2024/01/03/%E5%BC%82%E5%B8%B8%E6%A3%80%E6%B5%8B-%E4%B8%80-%E5%AF%BC%E8%AE%BA/"/>
    <url>/2024/01/03/%E5%BC%82%E5%B8%B8%E6%A3%80%E6%B5%8B-%E4%B8%80-%E5%AF%BC%E8%AE%BA/</url>
    
    <content type="html"><![CDATA[<h2 id="异常检测"><a href="#异常检测" class="headerlink" title="异常检测"></a>异常检测</h2><h3 id="异常"><a href="#异常" class="headerlink" title="异常"></a>异常</h3><h3 id="异常分类"><a href="#异常分类" class="headerlink" title="异常分类"></a>异常分类</h3><ul><li>点异常</li><li>连续性异常</li><li>上下文异常(监控场景)</li></ul><h3 id="异常检测应用场景"><a href="#异常检测应用场景" class="headerlink" title="异常检测应用场景"></a>异常检测应用场景</h3><h2 id="模型类型-Model-Type"><a href="#模型类型-Model-Type" class="headerlink" title="模型类型(Model Type)"></a>模型类型(Model Type)</h2><ul><li>有监督-DNN</li><li>半监督-oneClass-SVM</li><li>无监督-AutoEncoder</li><li>Hybrid</li></ul>]]></content>
    
    
    
    <tags>
      
      <tag>AIOPS</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>springboot微服务容器化流程</title>
    <link href="/2023/12/28/springboot%E5%BE%AE%E6%9C%8D%E5%8A%A1%E5%AE%B9%E5%99%A8%E5%8C%96%E6%B5%81%E7%A8%8B/"/>
    <url>/2023/12/28/springboot%E5%BE%AE%E6%9C%8D%E5%8A%A1%E5%AE%B9%E5%99%A8%E5%8C%96%E6%B5%81%E7%A8%8B/</url>
    
    <content type="html"><![CDATA[<h2 id="spring-cloud"><a href="#spring-cloud" class="headerlink" title="spring cloud"></a>spring cloud</h2><h2 id="kubernetes"><a href="#kubernetes" class="headerlink" title="kubernetes"></a>kubernetes</h2>]]></content>
    
    
    
    <tags>
      
      <tag>Microservice</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Machines论文笔记</title>
    <link href="/2023/12/18/Machines%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/"/>
    <url>/2023/12/18/Machines%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/</url>
    
    <content type="html"><![CDATA[<h2 id="The-Design-of-a-Practical-System-for-Fault-Tolerant-Virtual-Machines"><a href="#The-Design-of-a-Practical-System-for-Fault-Tolerant-Virtual-Machines" class="headerlink" title="The Design of a Practical System for Fault-Tolerant Virtual Machines"></a>The Design of a Practical System for Fault-Tolerant Virtual Machines</h2><h2 id="FAQ"><a href="#FAQ" class="headerlink" title="FAQ"></a>FAQ</h2><h3 id=""><a href="#" class="headerlink" title=""></a></h3>]]></content>
    
    
    
    <tags>
      
      <tag>distributed system</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>分布式文件系统相关概念导读</title>
    <link href="/2023/12/11/%E5%88%86%E5%B8%83%E5%BC%8F%E6%96%87%E4%BB%B6%E7%B3%BB%E7%BB%9F%E7%9B%B8%E5%85%B3%E6%A6%82%E5%BF%B5%E5%AF%BC%E8%AF%BB/"/>
    <url>/2023/12/11/%E5%88%86%E5%B8%83%E5%BC%8F%E6%96%87%E4%BB%B6%E7%B3%BB%E7%BB%9F%E7%9B%B8%E5%85%B3%E6%A6%82%E5%BF%B5%E5%AF%BC%E8%AF%BB/</url>
    
    <content type="html"><![CDATA[<h2 id="思考"><a href="#思考" class="headerlink" title="思考"></a>思考</h2><ul><li>存储系统如何保存应用状态？</li><li>如何为存储系统来设计它的高可用性与容错性？</li><li>如何控制失败次数？如何设计一个存储系统的SLA？</li><li>分布式存储系统如何在性能问题和一致性问题之间平衡？</li><li>分布式存储系统如何在容错性和一致性之间保持平衡？</li><li>CAP理论在分布式存储系统中起到什么样子的作用？</li><li>如何为K8S设计一个分布式存储系统？</li><li>S3存储系统是如何保证一致性，容错性等问题？</li><li>面对并发或者并行请求操作，分布式系统应该如何处理？</li><li>节点上磁盘介质的读写性能会有多大影响？</li><li>数据如何从内存到磁盘？又是怎么样从磁盘中恢复出来？</li></ul>]]></content>
    
    
    
    <tags>
      
      <tag>distributed system</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>GoogleFileSystem论文笔记</title>
    <link href="/2023/12/11/GoogleFileSystem%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/"/>
    <url>/2023/12/11/GoogleFileSystem%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/</url>
    
    <content type="html"><![CDATA[<h2 id="背景"><a href="#背景" class="headerlink" title="背景"></a>背景</h2><p>GFS(Google File System)是为了解决Google 数据处理需求快速增长的需求而发明的。它是一个大型数据密集型应用分布式文件系统，运行在廉价的商业硬件上。GFS 与以前的分布式文件系统有许多相同的目标，例如性能、可扩展性、可靠性和可用性。不同之处在于它提供了容错能力。</p><h2 id="GFS特点"><a href="#GFS特点" class="headerlink" title="GFS特点"></a>GFS特点</h2><h3 id="组件容错性"><a href="#组件容错性" class="headerlink" title="组件容错性"></a>组件容错性</h3><p>首先组成GFS系统成本不高，都是运行在廉价的商业硬件上，并且是上百数千台。所以有部分机器损坏也是正常。而引起损坏的有可能是:</p><ol><li>应用程序错误</li><li>操作系统错误</li><li>人为错误</li><li>以及磁盘、内存、连接器、网络和电源故障引起的问题。</li></ol><p>为了解决以上问题，需要使用以下手段:</p><ol><li>持续监控</li><li>错误检测</li><li>容错</li><li>自动恢复</li></ol><h3 id="数据处理以及文件操作设计"><a href="#数据处理以及文件操作设计" class="headerlink" title="数据处理以及文件操作设计"></a>数据处理以及文件操作设计</h3><p>因为需要面对的是数据爆炸增长，以TB组成的快速增长数据集， 以及在传统标准下文件会非常庞大，常见情况是存在多个GB( <strong>Multi-GB</strong>)大小文件。每个文件又包含了应用对象，比如说Web文档，这些不是普通二进制块，而是类似于JSON数据等等更高层次的应用数据结构。<strong>所以GFS重新设计了IO操作和块大小。</strong></p><h3 id="文件写入与读取"><a href="#文件写入与读取" class="headerlink" title="文件写入与读取"></a>文件写入与读取</h3><ol><li><strong>文件写入方式：</strong><ul><li><strong>GFS使用仅追加方式（only-append）：</strong> 在GFS中，文件的变更主要通过追加新数据到文件末尾的方式进行。这种追加方式是一种往文件中添加新数据而不是覆盖已存在数据的方式。这对于处理大规模数据的应用场景具有一定的优势。</li><li><strong>其他文件系统通常使用覆盖已存在数据的方式：</strong> 与GFS不同，其他一些传统的文件系统在进行文件写入时一般采用覆盖已存在数据的方式，即直接替换或修改文件中的某一部分数据。</li></ul></li><li><strong>随机写几乎不存在：</strong> GFS中随机写几乎不存在，文件的变更主要通过追加操作。这是因为大多数文件在GFS中是通过追加新数据而不是在文件中随机位置写入数据。</li><li><strong>文件的只读状态和顺序读取：</strong> 一旦文件被写入，就会被设置为只读状态，并且通常只以顺序读取的方式被访问。这强调了对于大文件来说，读取操作主要是按照文件的顺序进行的。</li><li><strong>追加作为性能优化和原子性保证的焦点：</strong> 由于文件的追加方式，GFS将性能优化和原子性保证的焦点放在了追加操作上。这是因为在大文件的访问模式中，追加操作是主要的写入方式，对其进行性能优化和保证原子性非常关键。</li><li><strong>客户端缓存数据块失去吸引力：</strong> 由于大文件主要以追加方式进行变更，并且很少涉及随机写入，因此在客户端缓存数据块方面失去了吸引力。这反映了对于GFS而言，优化追加操作的性能可能比优化客户端数据块缓存更为重要。</li></ol><h3 id="GFS设计和实现关键点"><a href="#GFS设计和实现关键点" class="headerlink" title="GFS设计和实现关键点"></a>GFS设计和实现关键点</h3><ol><li><strong>协同设计应用程序和文件系统API：</strong> GFS强调应用程序和文件系统API的协同设计，以提高整个系统的灵活性。这种协同设计有助于优化文件系统以满足特定应用程序需求，提高系统的性能和适应性。</li><li><strong>一致性模型的放宽：</strong> 为了简化文件系统而不给应用程序带来沉重的负担，GFS放宽了一致性模型。这是一种在一致性和性能之间的权衡，使得文件系统更为简单，同时满足系统的要求。</li><li><strong>原子追加操作的引入：</strong> GFS引入了原子追加操作，允许多个客户端同时向文件追加数据，而无需额外的同步。这提高了文件追加操作的并发性，有助于提高系统的性能。</li><li><strong>大规模部署：</strong> 描述了GFS目前在不同目的下部署了多个集群，其中最大的集群拥有超过1000个存储节点，超过300TB的磁盘存储，且由数百个客户端在不同机器上持续访问。</li></ol><h2 id="系统设计"><a href="#系统设计" class="headerlink" title="系统设计"></a>系统设计</h2><h3 id="假设"><a href="#假设" class="headerlink" title="假设"></a>假设</h3><ul><li>监控与容错:  GFS假设系统由许多廉价的通用组件组成，这些组件通常可能发生故障。因此，系统必须持续监测自身，能够检测、容忍并及时从组件故障中恢复。</li><li>存储大数据文件: 预期系统将存储几百万个文件，每个文件通常为100 MB或更大。多GB文件是常见情况，需要高效管理。尽管需要支持小文件，但不必为其进行优化。</li><li>工作负载读取方式<ul><li>大型流式读取: 涉及每个操作读取数百KB，更常见的是1MB或更多。</li><li>小型随机读取: 在文件的某个任意偏移位置读取少量KB。性能敏感的应用程序通常会批量处理和排序它们的小型读取，以稳定地通过文件前进。</li></ul></li><li>工作负载大型顺序写入</li><li>多客户端并发追加同一个文件: <strong>系统必须有效地实现多个客户端并发追加到相同文件的明确定义的语义</strong> 文件经常被用作生产者-消费者队列或多路合并。数百个生产者，每台机器运行一个，将同时追加到一个文件。原子性以及最小的同步开销是至关重要的。文件可以稍后读取，或者消费者可能同时读取文件。</li><li>高持续带宽比低延迟更重要: 大多数目标应用程序更注重以高速率批量处理数据，而对于单个读取或写入的响应时间要求不那么严格。</li></ul><h3 id="接口设计"><a href="#接口设计" class="headerlink" title="接口设计"></a>接口设计</h3><p>文件在目录中按层次结构组织，并由路径名标识。 支持创建、删除、打开、关闭、读取和写入文件的常规操作。<strong>需要注意的地方在于并没有按照POSIX等标准创建。</strong>并且它支持快照和记录追加操作。</p><p>Snapshot 以较低的成本创建文件或目录树的副本。 记录追加允许多个客户端同时将数据追加到同一个文件，同时保证每个客户端追加的原子性。</p><h3 id="系统架构"><a href="#系统架构" class="headerlink" title="系统架构"></a>系统架构</h3><p>GFS集群由一个master主节点和多个块服务器组成(chunkservers), 个节点通常是运行用户级服务器进程的通用Linux机器。并且支持多用户访问。</p><p>文件被划分为固定大小的块，每个块由主节点在块创建时分配的不可变且全局唯一的64位块句柄（chunk handle）标识。块服务器将块存储在本地磁盘上，作为Linux文件，并根据块句柄和字节范围读取或写入块数据。</p><p>为了提高可靠性，每个块在多个块服务器上进行复制，一般存储三个副本，但用户可以为文件命名空间的不同区域指定不同的复制级别。</p><p>master节点负责维护所有文件系统元数据，包括命名空间、访问控制信息、文件到块的映射以及块的当前位置。还有系统范围的活动，如块租约管理、孤立块的垃圾收集和块在块服务器之间的迁移。</p><p>GFS客户端代码嵌入到每个应用程序中，实现文件系统API，并与主节点和块服务器通信，代表应用程序读取或写入数据。客户端与主节点进行元数据操作，但所有承载数据的通信直接与块服务器进行。<strong>GFS不提供POSIX API，因此无需与Linux vnode层连接。</strong></p><p>客户端块服务器不缓存文件数据。<strong>客户端缓存对于大多数应用程序的大文件或工作集过大而无法缓存的情况效益不大。这样做简化了客户端和整个系统，消除了缓存一致性问题。块服务器也不需要缓存文件数据，因为块被存储为本地文件，因此Linux的缓冲缓存已经将经常访问的数据保存在内存中。</strong></p><p><img src="/../images/GFS/image-20231211112011692.png" alt="image-20231211112011692"></p><h3 id="master设计"><a href="#master设计" class="headerlink" title="master设计"></a>master设计</h3><p>设计成单master的原因在于，可以简化系统结构，它只需要接收客户端请求，告诉客户端块服务器地址即可。需要注意的是应该尽量不让master参与读写相关操作，容易成为瓶颈。</p><p>进行简单的读操作时，客户端首先使用固定的块大小将应用程序指定的文件名和字节偏移转换为文件中的块索引。然后，客户端向主节点发送一个请求，包含文件名和块索引。主节点回复包含相应的块句柄和副本的位置。客户端使用文件名和块索引作为键缓存这些信息。</p><p>客户端接着向其中一个副本发送请求，通常选择最近的副本。请求指定块句柄和块内的字节范围。对相同块的进一步读取不需要客户端与主节点的交互，直到缓存的信息过期或文件重新打开。参考架构图中(chunk handle,byte range)</p><p>客户端通常会在同一请求中请求多个块，而主节点也可以包含紧随所请求块之后的块的信息。这个额外的信息在几乎不增加额外成本的情况下，避免了未来的客户端-主节点交互。</p><h3 id="chunk大小设计"><a href="#chunk大小设计" class="headerlink" title="chunk大小设计"></a>chunk大小设计</h3><p>chunk大小选择在64MB，远大于典型的文件系统块大小。每个块副本存储为块服务器上的普通Linux文件，并根据需要进行扩展。采用懒惰的空间分配避免了由于内部碎片而浪费空间。而选择64MB的原因在于:</p><ol><li>减少与master节点交互频率： 因为在同一块上的读写只需要一次初始请求，获取块位置信息即可。对于大多数顺序读写大文件的应用程序，这一减少尤为显著。</li><li>降低网络开销: 大的chunk可以使客户端更有可能在给定的块上执行多个操作，<strong>通过在较长时间内保持到块服务器的持久TCP连接来降低网络开销。</strong></li><li>减小master节点上元数据大小: 大的chunk减少了存储在主节点上的元数据的大小，使得可以将元数据保持在内存中，从而带来其他优势。</li></ol><p>缺点在于:</p><ol><li>潜在的热点问题：大的chunk可能导致小文件只包含少量块，这可能使存储这些块的块服务器成为热点，特别是当许多客户端访问同一文件时。实际上，热点问题在实践中并不是主要问题，因为应用程序大多数情况下是顺序读取大型多块文件。</li><li>潜在的解决方案</li></ol><h3 id="元数据"><a href="#元数据" class="headerlink" title="元数据"></a>元数据</h3><p>master节点存放三种主要类型的元数据:</p><ul><li>文件和块的命名空间。</li><li>从文件到块的映射。</li><li>每个块副本的位置。</li></ul><p>所有的元数据都存储在内存中，前两种也可以通过将变更记录到主节点本地磁盘上的操作日志来保持持久性，并在远程机器上进行复制。</p><p>主节点不会存储给定块的哪些块服务器有副本的持久记录。它在启动时简单地向块服务器轮询该信息。主节点保持自身的状态更新，因为它控制所有块的放置并通过定期的HeartBeat消息监视块服务器的状态。</p><p>初始尝试将块位置信息持久保存在主节点上，但决定在启动时从块服务器请求数据，以及之后定期请求。这消除了在块服务器加入和离开集群、更改名称、故障、重启等情况下保持主节点和块服务器同步的问题。</p><h3 id="一致性模型"><a href="#一致性模型" class="headerlink" title="一致性模型"></a>一致性模型</h3><p>Google File System (GFS)采用的是一种松散的一致性模型，以支持其高度分布式的应用场景:</p><ul><li><strong>命名空间变更的原子性：</strong><ul><li>文件命名空间的变更（例如文件创建）是原子的，并由主节点专属处理。</li><li>命名空间锁定保证了原子性和正确性，主节点的操作日志定义了这些操作的全局总序列。</li></ul></li><li><strong>文件区域状态和数据变更：</strong><ul><li>数据变更的结果取决于变更的类型、是否成功以及是否存在并发变更。</li><li>表格1概述了数据变更后文件区域的状态。</li><li>一致的区域意味着所有客户端将始终看到相同的数据，定义的区域是一致的，并且客户端将看到变更写入的完整数据。</li></ul></li><li><strong>数据变更类型：</strong><ul><li>数据变更可以是写入或记录追加。写入在应用程序指定的文件偏移处写入数据。记录追加在至少一次原子地追加数据（“记录”），即使存在并发变更，但在GFS选择的偏移处进行（第3.3节）。</li></ul></li><li><strong>成功变更的保证：</strong><ul><li>在一系列成功的变更之后，变更的文件区域保证是定义的，并包含由最后一个变更写入的数据。</li><li>GFS通过在所有副本上相同顺序应用变更（第3.1节）和使用块版本号来检测已变得陈旧的副本（第4.5节）来实现这一点。</li><li>陈旧的副本永远不会参与变更或提供给向主节点请求块位置的客户端。它们在最早的机会被垃圾回收。</li></ul></li><li><strong>缓存和数据一致性：</strong><ul><li>由于客户端缓存块位置，它们在刷新信息之前可能从一个陈旧的副本读取。这个窗口受缓存条目的超时和文件的下一次打开限制。</li><li>大多数文件是仅追加的，因此陈旧的副本通常会返回早于数据的结束而不是过时的数据。当读者重试并联系主节点时，它将立即获得当前的块位置。</li></ul></li><li><strong>组件故障和数据一致性：</strong><ul><li>长时间之后，成功的变更后，组件故障仍然可能会损坏或销毁数据。</li><li>GFS通过主节点与所有块服务器之间的定期握手识别故障的块服务器，并通过校验和检测数据损坏（第5.2节）。</li><li>一旦问题出现，数据将尽快从有效副本中恢复（第4.3节）。</li><li>仅当所有副本在GFS能够做出反应之前全部丢失时，块才会不可逆地丢失。即使在这种情况下，它会变得不可用，而不是损坏：应用程序会收到明确的错误而不是损坏的数据。</li></ul></li></ul><p><img src="/../images/GFS/image-20231212105830705.png" alt="image-20231211165134518"></p><p>应用程序通过以下技术来适应松散的一致性模型。主要的适应技术包括依赖于追加而不是覆盖、定期的检查点操作，以及编写自验证、自识别记录。</p><p><strong>主要观点和技术：</strong></p><ol><li><strong>使用追加而非覆盖：</strong> GFS应用程序中几乎所有的文件变更操作都是通过追加而不是覆盖来完成的。这种方式更加高效且更具弹性，特别是对于大规模的数据写入，追加操作更容易适应并发。</li><li><strong>定期的检查点操作：</strong> 应用程序可以定期进行检查点操作，记录文件的当前状态。这样的检查点可以包含应用级别的校验和信息，用于验证数据的完整性。读取器只需处理到最后一个检查点的文件区域，确保处理的数据处于定义状态。</li><li><strong>自验证、自识别的记录：</strong> 当多个写入者并发追加到文件时，记录追加的“至少一次”语义保留了每个写入者的输出。记录中包含额外信息（如校验和），使得读取器可以验证记录的有效性，并处理可能的填充和重复。如果读取器不能容忍偶尔的重复，可以使用记录中的唯一标识符进行过滤。</li><li><strong>记录的顺序和去重：</strong> 应用程序中的记录写入操作通常按照特定的顺序进行，并使用记录中的唯一标识符确保顺序和去重。这样，最终交付给记录读取器的是相同顺序的记录序列，除了偶尔的重复。</li></ol><h2 id="系统交互"><a href="#系统交互" class="headerlink" title="系统交互"></a>系统交互</h2><p>这一节主要是介绍client是如何和master以及chunk server交互的过程，涉及到以下几个操作:</p><ul><li>数据变更</li><li>原子追加</li><li>快照</li></ul><p>需要注意，应该减少master参与以上操作。</p><h3 id="Leases-and-Mutation-Order-租约与变更顺序"><a href="#Leases-and-Mutation-Order-租约与变更顺序" class="headerlink" title="Leases and Mutation Order(租约与变更顺序)"></a>Leases and Mutation Order(租约与变更顺序)</h3><h4 id="租约："><a href="#租约：" class="headerlink" title="租约："></a>租约：</h4><ul><li>租约用于保持所有副本之间一致的变更顺序。</li><li>主节点授予租约给一个副本，称为主副本，该副本为变更选择一个序列顺序。</li><li>所有副本都按照此顺序进行变更，定义了全局的变更顺序。</li><li>主节点授予一个块租约给其中一个副本，被称为主副本（primary）。</li><li>主副本选择了一种序列顺序，用于所有对该块的变更。</li><li>所有副本都按照主副本确定的序列顺序应用变更，确保全局的变更顺序。</li><li>租约的授予顺序由主节点决定，而在租约内部，由主副本分配的序列号决定。</li></ul><h4 id="租约管理："><a href="#租约管理：" class="headerlink" title="租约管理："></a>租约管理：</h4><ul><li>租约初始超时为60秒。</li><li>只要块在变更，主副本可以请求并通常在主节点上无限期地接收租约延期。</li><li>租约延期请求和授予是通过主节点和所有块服务器之间定期交换的心跳消息进行的。</li><li>主节点有时可能尝试在租约到期之前撤销租约，例如，当主节点希望在正在重命名的文件上禁用变更时。</li><li>即使主节点失去与主副本的通信，它仍可以在旧租约过期后安全地向另一个副本授予新的租约。</li></ul><h4 id="租约撤销："><a href="#租约撤销：" class="headerlink" title="租约撤销："></a>租约撤销：</h4><ul><li>主节点有时可能在租约到期之前尝试撤销租约（例如，当禁用正在重命名的文件上的变更时）。</li><li>即使主节点与主副本失去通信，它也可以在旧租约到期后安全地向另一个副本授予新的租约。</li></ul><h4 id="数据写入流程"><a href="#数据写入流程" class="headerlink" title="数据写入流程:"></a>数据写入流程:</h4><ol><li>请求租约信息，确定当前租约的主副本和其他副本的位置。</li><li>master回复主副本和其他副本位置，客户端缓存此信息来提供未来变更。</li><li>将数据推送到所有副本，每个副本都在内部的LRU缓冲区中存储数据，以提高性能。</li><li>一旦所有副本都确认收到数据，客户端向主副本发送写入请求。</li><li>主副本为接收到的所有变更分配连续的序列号，将变更应用到本地状态中。</li><li>主副本将写入请求转发给所有次要(<strong>Secondary</strong>)副本，每个次要副本按主副本分配的序列号应用变更。</li><li>所有次要副本回复主副本，表示它们已完成操作。</li><li>主副本回复客户端，汇报任何在任何副本上遇到的错误。</li></ol><p><img src="/../images/GFS/image-20231212105830705.png" alt="image-20231212105830705"></p><h4 id="错误处理"><a href="#错误处理" class="headerlink" title="错误处理"></a>错误处理</h4><ul><li>如果在任何副本上遇到错误，客户端会收到错误报告。</li><li>如果写入在主副本和部分次要副本上成功，但在其他副本上失败，客户端请求被视为失败，修改的区域处于不一致状态。</li><li>客户端代码通过在写入失败时重试来处理此类错误。如果步骤（3）到（7）之间的某个步骤失败，客户端将在写入的开始处重试。</li></ul><h4 id="大型写入操作"><a href="#大型写入操作" class="headerlink" title="大型写入操作"></a>大型写入操作</h4><ul><li>对于大型写入操作，GFS客户端代码将其分解为多个写入操作，按照上述控制流程进行。</li><li>写入操作可能会与其他客户端的并发操作相互交错，因此共享的文件区域可能包含来自不同客户端的片段，尽管副本是相同的。</li></ul><h3 id="数据流"><a href="#数据流" class="headerlink" title="数据流"></a>数据流</h3><p>这个没什么太多重点，数据流和控制流分开，这样做的好处是可以最大程度利用网络带宽。每次数据流动都是选择距离自己最近的节点。GFS使用TCP连接来最大程度来减少延迟，一旦chunkserver接收一些数据就会立马转发。因为使用的是全双工链路的交换网络。 立即发送数据不会降低接收速率。</p><h3 id="原子追加-Atomic-Record-Appends"><a href="#原子追加-Atomic-Record-Appends" class="headerlink" title="原子追加(Atomic Record Appends)"></a>原子追加(Atomic Record Appends)</h3><p>传统写操作需要指定文件偏移量才能写入，这对于分布式系统来说是不能接受的，因为成本太高了，需要引入分布式锁以及同步等资源消耗。但是在GFS中使用记录追加操作，客户端仅指定数据。 GFS将其至少原子地附加到文件一次。</p><p>并将该偏移量返回给客户端。这类似于在Unix中写入一个以O APPEND模式打开的文件，而没有多个写入器并发执行的竞争条件。</p><p>操作流程如下：</p><ol><li>客户端询问主服务器关于某块的租约和其他副本的位置。</li><li>主服务器回复主副本的身份和其他副本的位置。</li><li>客户端将数据推送到所有副本。</li><li>所有副本确认接收数据后，客户端向主副本发送写请求，标识之前推送的数据。</li><li>主副本为接收到的所有突变分配连续的序列号，按照序列号的顺序将其应用于其本地状态。</li><li>主副本将写请求转发给所有次要副本，次要副本按照主副本分配的序列号顺序应用突变。</li><li>所有次要副本回复主副本，表示它们已完成操作。</li><li>主副本回复客户端，报告操作的成功或错误。</li></ol><p>对于”record append”，如果记录的大小超过了块的最大限制(64MB)，主副本会将块填充到最大大小，通知次要副本执行相同的操作，并告知客户端在下一个块上重试操作。如果记录适合最大大小，主副本将数据追加到其副本，通知次要副本在确切的偏移量上写入数据，并向客户端回复成功。</p><p>“record append”保证了数据至少以原子单元的形式写入，即使在不同副本上，但不保证所有副本在字节级别上相同。这种设计使得GFS可以高效地处理许多客户端在不同机器上同时追加到同一文件的分布式应用场景</p><h3 id="快照-Snapshot"><a href="#快照-Snapshot" class="headerlink" title="快照(Snapshot)"></a>快照(Snapshot)</h3><p>Snapshot使用标准的写时复制方式实现快照，像AFS一样。</p><p>通常用于快速创建大型数据集的分支副本，或在可以轻松提交或回滚的情况下进行实验性更改前创建当前状态的检查点。</p><p>操作过程如下：</p><ol><li><strong>撤销租约：</strong> 当主服务器接收到快照请求时，首先撤销文件中即将快照的块上的所有未完成的租约。撤销租约确保对这些块的后续写入将需要与主服务器的交互以查找租约持有者，从而给主服务器一个创建块的新副本的机会。</li><li><strong>记录操作：</strong> 在撤销或等待租约到期后，主服务器将快照操作记录到磁盘。</li><li><strong>复制元数据：</strong> 主服务器通过复制源文件或目录树的元数据来应用日志记录到其内存状态。新创建的快照文件指向与源文件相同的块。</li><li><strong>创建新块：</strong> 当客户端在快照后想要写入块时，它向主服务器发送请求以查找当前的租约持有者。主服务器注意到块的引用计数大于1，推迟回复客户端的请求并选择一个新的块句柄（C’）。主服务器要求每个具有块C当前副本的块服务器创建一个名为C’的新块。这确保新块可以在与原始块相同的块服务器上本地创建，避免了网络传输。</li><li><strong>租约和回复：</strong> 主服务器向新块（C’）的一个副本授予租约，并回复客户端。现在，客户端可以正常写入块，而不知道它刚刚是从现有块创建的。</li></ol><h2 id="Master操作"><a href="#Master操作" class="headerlink" title="Master操作"></a>Master操作</h2><p>master执行所有namespace操作，同时还管理整个系统中的块副本：它做出放置决策，创建新的块和副本，并协调各种系统范围的活动以保持块完全复制，平衡所有块服务器的负载，并回收未使用的存储。</p><h3 id="namespace管理与锁"><a href="#namespace管理与锁" class="headerlink" title="namespace管理与锁"></a>namespace管理与锁</h3><p>GFS在逻辑上用一个完整路径名到元数据的查找表来表示命名空间。通过前缀压缩技术，这个查找表可在内存中高效地表示。在命名空间树上的每个节点（既可能是一个文件的绝对路径名，也可能是一个目录的绝对路径名）都有一个与之关联的读写锁（read-write lock）。</p><ol><li><strong>长时间操作的处理：</strong><ul><li>某些master操作可能需要很长时间，例如快照操作需要撤销所有涉及到快照范围内的chunk的租约。为了不延迟其他master操作，允许多个操作同时进行。</li></ul></li><li><strong>命名空间表示：</strong><ul><li>GFS的命名空间没有像传统文件系统那样的每个目录都列出其中所有文件的数据结构。也不支持相同文件或目录的别名（即Unix术语中的硬链接或符号链接）。</li><li>GFS在逻辑上将其命名空间表示为将完整路径映射到元数据的查找表。通过前缀压缩，可以在内存中高效表示这个表。</li></ul></li><li><strong>锁的分配：</strong><ul><li>每个命名空间树节点（绝对文件名或绝对目录名）都有一个相关的读写锁。</li><li>每个master操作在运行之前会获取一组锁。通常，如果涉及到路径&#x2F;d1&#x2F;d2&#x2F;…&#x2F;dn&#x2F;leaf，它将获取目录名&#x2F;d1，&#x2F;d1&#x2F;d2，…，&#x2F;d1&#x2F;d2&#x2F;…&#x2F;dn的读锁，以及&#x2F;d1&#x2F;d2&#x2F;…&#x2F;dn&#x2F;leaf的读锁或写锁。</li></ul></li><li><strong>锁的冲突处理：</strong><ul><li>锁的机制可以防止在对&#x2F;home&#x2F;user进行快照到&#x2F;save&#x2F;user的过程中创建文件&#x2F;home&#x2F;user&#x2F;foo。这是通过在快照操作上获取对&#x2F;home和&#x2F;save的读锁，以及对&#x2F;home&#x2F;user和&#x2F;save&#x2F;user的写锁，以及在文件创建操作上获取对&#x2F;home和&#x2F;home&#x2F;user的读锁，以及对&#x2F;home&#x2F;user&#x2F;foo的写锁来实现的。</li></ul></li><li><strong>并发修改的处理：</strong><ul><li>此锁定方案允许在同一目录中进行并发的变更操作。例如，可以同时执行多个文件创建操作：每个操作都在目录名上获取读锁，并在文件名上获取写锁。目录名的读锁足以防止目录被删除、重命名或进行快照。文件名的写锁用于串行化尝试使用相同名称创建文件的操作。</li></ul></li><li><strong>锁的懒惰分配和一致性排序：</strong><ul><li>由于命名空间可能有许多节点，锁对象是惰性分配的，并在不再使用时被删除。</li><li>锁是按照一致的总序列顺序获取，以防止死锁：首先按照命名空间树中的级别排序，然后在相同级别内按字典顺序排序。</li></ul></li></ol><h3 id="副本分配"><a href="#副本分配" class="headerlink" title="副本分配"></a>副本分配</h3><ol><li><strong>集群分布：</strong><ul><li>GFS集群通常具有数百个chunkservers，分布在许多机架上。这些chunkservers又可以被来自相同或不同机架的数百个客户端访问。</li><li>不同机架上的两台机器之间的通信可能要穿越一个或多个网络交换机。此外，机架内进出的带宽可能小于机架内所有机器的总带宽。</li></ul></li><li><strong>分布挑战：</strong><ul><li>多级分布对数据的分发提出了挑战，需要在多个层面上实现可扩展性、可靠性和可用性。</li></ul></li><li><strong>复制策略目标：</strong><ul><li>复制策略有两个目标：最大化数据的可靠性和可用性，以及最大化网络带宽的利用率。</li><li>仅仅将副本分布在不同的机器上是不够的，这只是为了防范磁盘或机器故障，并充分利用每台机器的网络带宽。</li><li>必须同时将chunk副本分布在不同的机架上，以确保即使整个机架受损或离线（例如，由于共享资源（如网络交换机或电源电路）的故障），某些chunk的副本仍然存活并保持可用性。</li><li>这也意味着对于一个chunk的流量，特别是读取流量，可以利用多个机架的总带宽。然而，写入流量必须流经多个机架，这是一个权衡，但GFS选择了这种方式。</li></ul></li></ol><h3 id="chunk创建，重做副本，重均衡"><a href="#chunk创建，重做副本，重均衡" class="headerlink" title="chunk创建，重做副本，重均衡"></a>chunk创建，重做副本，重均衡</h3><p>chunk副本的创建可能由三个原因引起：</p><ul><li>chunk创建</li><li>重做副本（re-replication）</li><li>重均衡（rebalance）</li></ul><h4 id="chunk创建"><a href="#chunk创建" class="headerlink" title="chunk创建"></a>chunk创建</h4><p>当master创建一个chunk时，它选择在哪里放置最初为空的副本，以下是它的考虑因素:</p><ul><li>在磁盘空间利用率低于平均水平的chunkservers上放置新副本，以实现磁盘利用率的均衡。</li><li>限制每个chunkserver上的“最近”创建的数量，因为创建本身虽然廉价，但它可靠地预测到即将发生的大量写流量。</li><li>如前所述，确保chunk的副本分布在不同的机架上。</li></ul><h4 id="重做副本"><a href="#重做副本" class="headerlink" title="重做副本"></a>重做副本</h4><ul><li>当可用副本数量低于用户指定的目标时，master会立即重新复制一个chunk。</li><li>可能的原因包括：一个chunkserver变得不可用，报告其副本可能已损坏，其磁盘由于错误而被禁用，或者增加了复制目标。</li><li>需要重新复制的每个chunk都基于多个因素进行了优先级排序，例如距离其复制目标的距离，文件是否处于活动状态等。</li></ul><h4 id="重均衡"><a href="#重均衡" class="headerlink" title="重均衡"></a>重均衡</h4><ul><li>master定期对副本进行重新平衡：它检查当前副本分布，并移动副本以实现更好的磁盘空间和负载平衡。</li><li>通过这个过程，master逐渐填充一个新的chunkserver，而不是立即用新的chunk淹没它以及伴随着它们的大量写流量。</li><li>新副本的放置标准类似于上面讨论的标准。此外，master还必须选择要删除的现有副本，通常优先删除在chunkservers上的副本，以实现磁盘空间使用的均衡。</li></ul><h3 id="垃圾收集"><a href="#垃圾收集" class="headerlink" title="垃圾收集"></a>垃圾收集</h3><p>在文件被删除后，GFS不会立即回收可用的物理存储空间，而是在文件和chunk两个层面上定期进行垃圾收集。这种方法使系统更加简单和可靠。</p><h4 id="回收机制"><a href="#回收机制" class="headerlink" title="回收机制"></a>回收机制</h4><ol><li>当应用程序删除文件时，master会立即记录删除操作，并将文件重命名为包含删除时间戳的隐藏名称。</li><li>定期扫描命名空间，master删除已存在超过三天的隐藏文件。在此期间，文件可以通过新的特殊名称读取，并可以通过将其重新命名回正常状态来取消删除。</li><li>当隐藏文件从命名空间中删除时，其内存中的元数据被清除，切断了与所有chunk的链接。</li><li>在定期扫描chunk命名空间时，master识别并删除孤立的chunks（不可从任何文件访问到的chunks）。</li></ol><h4 id="回收机制讨论"><a href="#回收机制讨论" class="headerlink" title="回收机制讨论"></a>回收机制讨论</h4><ol><li><strong>简单而可靠：</strong> 相较于急切的删除，垃圾收集方法在分布式系统中更为简单可靠。它提供了一种一致和可靠的方式来清理未知有用的副本。</li><li><strong>合并到后台活动：</strong> 垃圾收集将存储回收合并到master的常规后台活动中，例如对命名空间的定期扫描和与chunkservers的握手。这有助于批量处理，成本分摊，而且只在master相对空闲时执行。</li><li><strong>提供安全保障：</strong> 推迟回收存储提供了一种安全保障，防止意外和不可逆的删除。文件仍可以在被删除后的一段时间内读取，并在需要时可以重新使用。</li><li><strong>用户灵活性：</strong> 允许用户对不同命名空间部分应用不同的复制和回收策略，使用户能够根据需求进行定制。</li></ol><h3 id="过期副本检测机制-Stale-Replica-Detection"><a href="#过期副本检测机制-Stale-Replica-Detection" class="headerlink" title="过期副本检测机制( Stale Replica Detection)"></a>过期副本检测机制( Stale Replica Detection)</h3><ol><li><strong>Chunk版本号：</strong> 每个chunk在master上都有一个版本号，用于区分最新的副本和陈旧的副本。</li><li><strong>租约分配时更新版本号：</strong> 每当master分配一个新的chunk租约时，它会增加chunk版本号并通知所有最新的副本。在客户端开始写入chunk之前，这些副本和master都会将新的版本号记录在它们的持久状态中。</li><li><strong>检测陈旧副本：</strong> 如果一个副本的chunkserver当前不可用，它的chunk版本号将不会增加。当chunkserver重新启动并报告其chunks及其关联版本号时，master会检测到该chunkserver有一个陈旧的副本。</li><li><strong>陈旧副本的处理：</strong> master会在定期的垃圾收集中移除陈旧的副本。在此之前，当客户端请求chunk信息时，master会在回复中将陈旧的副本视为根本不存在。为了增强安全性，master在通知客户端某个chunk由哪个chunkserver持有租约时，或在在克隆操作中指示一个chunkserver从另一个chunkserver读取chunk时，会包含chunk版本号。客户端或chunkserver在执行操作时验证版本号，以确保始终访问最新的数据。</li></ol><h2 id="容错性与诊断"><a href="#容错性与诊断" class="headerlink" title="容错性与诊断"></a>容错性与诊断</h2><p>GFS面临的主要挑战之一就是如何处理频繁组件故障，由于机器和磁盘的质量和数量，这些问题更常见而非例外：我们不能完全信任机器，也不能完全信任磁盘。组件故障可能导致系统不可用，甚至更糟，导致数据损坏。</p><p>以下是GFS如何应对这些挑战以及系统中集成的用于诊断问题的工具：</p><p><strong>容错性：</strong></p><ol><li><strong>副本：</strong> GFS通过在多个chunkservers上存储相同数据的多个副本来提高容错性。如果一个chunkserver发生故障，系统仍然可以从其他副本中获取数据。</li><li><strong>Lease机制：</strong> 使用租约机制确保在写入操作期间保持一致性。当master给一个chunkserver颁发租约时，该chunkserver成为主要的，负责处理所有关于该chunk的写入请求。即使其他副本所在的chunkserver发生故障，主要副本仍然可用，确保一致性。</li></ol><p><strong>诊断工具：</strong></p><ol><li><strong>日志记录：</strong> GFS通过详细的日志记录来追踪系统中的各种活动。这使得在发生故障时可以审查日志以找到问题的根本原因。</li><li><strong>版本号和租约：</strong> GFS使用版本号和租约机制来检测和处理陈旧副本。这有助于追踪副本的状态并确保系统不会使用陈旧的数据。</li><li><strong>周期性的垃圾收集：</strong> 在定期的垃圾收集过程中，master会识别和删除不再需要的副本。这有助于释放存储空间并维护系统的整体健康状态。</li><li><strong>HeartBeat消息：</strong> 定期的HeartBeat消息允许master了解每个chunkserver的状态。如果一个chunkserver长时间没有响应，master可以将其标记为故障，并采取相应的措施。</li></ol><h3 id="高可用机制"><a href="#高可用机制" class="headerlink" title="高可用机制"></a>高可用机制</h3><p>过两个简单而有效的策略保持整个系统的高度可用性：快速恢复(fast recovery)和复制(replication)。</p><h4 id="快速恢复-fast-recovery"><a href="#快速恢复-fast-recovery" class="headerlink" title="快速恢复(fast recovery)"></a>快速恢复(fast recovery)</h4><p>无论master还是chunkserver，都设计成在几秒钟内恢复其状态并重新启动，无论它们如何终止。并不会区分是什么原因导致，直接通过终止进程来关闭。客户端和其他服务器在其未完成的请求上超时，重新连接到重新启动的服务器并重试时会经历短暂的中断。</p><h4 id="复制"><a href="#复制" class="headerlink" title="复制"></a>复制</h4><ul><li>chunk副本</li><li>master副本</li></ul><p>每个chunk在不同机架的多个chunkserver上都有副本。用户可以为文件命名空间的不同部分指定不同的副本级别，默认为三个。主要克隆现有副本，以确保每个chunk在chunkservers下线或通过校验和验证检测到损坏的副本时都能够完全复制。</p><p>为了提高可靠性，主节点的状态进行了复制。其操作日志和检查点在多台机器上进行了复制。对状态的变更只有在其日志记录已在本地磁盘上和所有主节点副本上刷新之后才被视为已提交。为了简化起见，一个主节点进程负责所有变更，以及改变系统内部的后台活动，如垃圾回收。当主节点失败时，它可以几乎立即重新启动。如果其机器或磁盘发生故障，则GFS外部的监控基础设施将在具有复制的操作日志的其他地方启动新的主节点进程。</p><p>此外，“shadow”主节点在主节点关闭时提供对文件系统的只读访问。它们是阴影而不是镜像，因为它们可能稍微滞后于主节点，通常是几分之一秒。它们增强了对不活跃进行变异的文件或不介意获得略旧结果的应用程序的读取可用性。实际上，由于文件内容是从chunkservers中读取的，应用程序不会观察到过时的文件内容。在短时间窗口内可能过时的是文件元数据，例如目录内容或访问控制信息。</p><p>为了保持自身的最新状态，阴影主节点读取操作日志的副本，并对其数据结构应用与主节点完全相同的变更序列。与主节点一样，它在启动时轮询chunkservers（以及之后不经常）以查找chunk副本，并与它们交换频繁的握手消息以监视它们的状态。它仅依赖于主要主节点，以获取由于主要的决定而导</p><h3 id="数据完整性"><a href="#数据完整性" class="headerlink" title="数据完整性"></a>数据完整性</h3><p>在GFS中，为了保障数据的完整性，每个chunkserver使用校验和来检测存储数据的损坏。校验和计算基于64 KB的块，并通过在内存中和日志记录中持久存储校验和，与用户数据分开。这有助于在读取过程中验证数据块的完整性，并防止损坏的数据传播到其他机器。在发现损坏时，系统通过其他副本进行恢复，并删除损坏的副本。</p><p>在写入追加数据时，校验和的计算被优化，只需增量更新最后一个部分的校验和块，以及计算由追加填充的任何新的校验和块的新校验和。这样即使最后一个部分校验和块已经损坏，系统也可以在下次读取时检测到损坏。</p><p>此外，系统通过定期扫描和验证不活动chunk的内容，发现在很少读取的chunk中可能存在的损坏。一旦检测到损坏，master可以创建一个新的未损坏的副本并删除已损坏的副本，以防止不活动但已损坏的chunk副本影响系统的正确性。</p><p>在诊断方面，GFS采用广泛和详细的诊断日志记录。这些日志记录重要事件和所有RPC请求和响应，有助于隔离问题、调试系统以及进行性能分析。由于这些日志是顺序异步写入的，因此对性能几乎没有影响。它们还允许在匹配请求与响应的情况下，整理记录的RPC，以重建系统的整个交互历史，以诊断问题。</p><h2 id="例子"><a href="#例子" class="headerlink" title="例子"></a>例子</h2><h3 id="文件追加"><a href="#文件追加" class="headerlink" title="文件追加"></a>文件追加</h3><p>使用golang对比文件追加和文件覆盖两种方式在并发情况下的表现:</p><p><strong>文件覆盖</strong></p><p><img src="/../images/GFS/image-20231211174452339.png" alt="image-20231211174452339"></p><p><strong>仅追加</strong></p><p><img src="/../images/GFS/image-20231211173612905.png" alt="image-20231211173612905"></p><h2 id="总结与思考"><a href="#总结与思考" class="headerlink" title="总结与思考"></a>总结与思考</h2><p>GFS帮助我们解决了大数据增长情况下的数据存储，但是在并没有进行小文件相关优化。在系统架构方面，采用了松散一致性模型，单master和多chunk server的架构设计。同时加入了多副本机制来保证数据的完整性。并且将数据拆分为64MB，比一般的文件系统的块都要大，这样做的好处在于可以最大程度的利用网络带宽。在数据写入方面，并没有使用覆盖的方式，而是使用了追加操作，这样做的性能更高。</p><ul><li><strong>GFS解决了什么问题？</strong><ul><li>GFS解决了大规模数据存储和处理的问题，特别是在Google的分布式计算环境中。它旨在提供高可用性、容错性和高性能的分布式文件系统，以支持大规模数据处理工作负载。</li></ul></li><li><strong>GFS系统架构设计？</strong><ul><li>GFS采用了主从架构，由一个Master节点和多个Chunk Server节点组成。Master负责维护文件元数据和命名空间，而Chunk Server存储实际的数据块。</li></ul></li><li><strong>在GFS中，对一个不存在的文件写入会发生什么？对一个存在的文件进行写入是什么样子的？</strong><ul><li>对不存在的文件写入将创建该文件。对已存在的文件进行写入将追加数据到文件末尾，采用的是append操作，而不是覆盖。</li></ul></li><li><strong>在GFS中，对文件进行读操作的时间复杂度是多少？</strong><ul><li>GFS采用了顺序读取和大块数据的方式，使得文件的读取效率很高。时间复杂度主要受到磁盘I&#x2F;O的影响，通常为O(1)或O(log N)。</li></ul></li><li><strong>GFS如何拆分大文件？并且如何检索被拆分后的文件？</strong><ul><li>GFS将大文件切分成固定大小的块（默认64MB），每个块分配一个唯一的块标识符。文件的元数据中包含了块的顺序和块标识符，从而实现了对文件的拆分和检索。</li></ul></li><li><strong>为什么GFS选用64MB作为块单位？</strong><ul><li>64MB的块大小在提高数据读写效率和最大程度利用网络带宽之间取得了平衡。大块减少了元数据的负担，同时也降低了网络开销。</li></ul></li><li><strong>版本号的作用是什么？为什么需要版本号？</strong><ul><li>版本号用于标识每个数据块的版本，确保数据的一致性。版本号的引入使得GFS能够区分最新的块副本，并在发生错误或冲突时进行正确的处理。</li></ul></li><li><strong>为什么GFS使用append？</strong><ul><li>GFS使用append操作而不是覆盖写，这种方式更适合Google的工作负载，特别是在大规模数据追加的场景下，避免了多个写操作之间的冲突和同步问题。</li></ul></li><li><strong>客户端本地缓存如何保证数据一致性？</strong><ul><li>GFS客户端本地缓存采用了“读多写少”的策略，不强制要求缓存与服务器的数据保持一致。在写入操作后，客户端通过通知Master来使得缓存无效，从而保证一致性。在读操作时，客户端会检查缓存是否过期或无效，必要时会从服务器重新获取数据。</li></ul></li></ul>]]></content>
    
    
    
    <tags>
      
      <tag>distributed system</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>k8s日志收集方案调研</title>
    <link href="/2023/12/09/k8s%E6%97%A5%E5%BF%97%E6%94%B6%E9%9B%86%E6%96%B9%E6%A1%88%E8%B0%83%E7%A0%94/"/>
    <url>/2023/12/09/k8s%E6%97%A5%E5%BF%97%E6%94%B6%E9%9B%86%E6%96%B9%E6%A1%88%E8%B0%83%E7%A0%94/</url>
    
    <content type="html"><![CDATA[<h2 id="背景"><a href="#背景" class="headerlink" title="背景"></a>背景</h2><p>之前每次查看日志的时候都是使用logs -f对应容器这样去查看，这样的好处是简单，但是每次都这样去弄太麻烦了，而且日志一多起来查找起来也非常麻烦，所以正好借此机会做一次日志收集平台展示调研方案。</p><h2 id="需求"><a href="#需求" class="headerlink" title="需求"></a>需求</h2><p>因为我是个人集群，所以在集群配置上不会很高，节点也不会有很多，是一个标准三节点，同时上面还跑着其他中间件等等。所以对资源消耗大的日志收集方案就不考虑，再一个是我的日志不需要保留很长时间，1-3天日志就完全够了。像ELK这种就太重了，虽然K8S集群上有一套ES集群了。总结一下就是以下需求:</p><ul><li>轻量级，对资源要求不高。</li><li>日志存放时间不需要太长。</li><li>对查询友好，能够准确做到像logs -f xxx一样，并且可以显示重点信息。</li></ul><h2 id="收集方案"><a href="#收集方案" class="headerlink" title="收集方案"></a>收集方案</h2><h3 id="EFK-Elasticsearch-Filebeat-Kibana"><a href="#EFK-Elasticsearch-Filebeat-Kibana" class="headerlink" title="EFK(Elasticsearch&#x2F;Filebeat&#x2F;Kibana)"></a>EFK(Elasticsearch&#x2F;Filebeat&#x2F;Kibana)</h3><h4 id="优点"><a href="#优点" class="headerlink" title="优点"></a>优点</h4><h4 id="缺点"><a href="#缺点" class="headerlink" title="缺点"></a>缺点</h4><h4 id="资源消耗"><a href="#资源消耗" class="headerlink" title="资源消耗"></a>资源消耗</h4><p>可以看到资源占用率非常高:</p><p><img src="C:\Users\wuliang\AppData\Roaming\Typora\typora-user-images\image-20231209192802879.png" alt="image-20231209192802879"></p><p><img src="C:\Users\wuliang\AppData\Roaming\Typora\typora-user-images\image-20231209195748655.png" alt="image-20231209195748655"></p><h3 id="loki"><a href="#loki" class="headerlink" title="loki"></a>loki</h3><h4 id="优点-1"><a href="#优点-1" class="headerlink" title="优点"></a>优点</h4><h4 id="缺点-1"><a href="#缺点-1" class="headerlink" title="缺点"></a>缺点</h4><h4 id="资源消耗-1"><a href="#资源消耗-1" class="headerlink" title="资源消耗"></a>资源消耗</h4><h3 id="FC-Filebeat-clickhouse"><a href="#FC-Filebeat-clickhouse" class="headerlink" title="FC(&#x2F;Filebeat&#x2F;clickhouse)"></a>FC(&#x2F;Filebeat&#x2F;clickhouse)</h3><h4 id="优点-2"><a href="#优点-2" class="headerlink" title="优点"></a>优点</h4><h4 id="缺点-2"><a href="#缺点-2" class="headerlink" title="缺点"></a>缺点</h4><h4 id="资源消耗-2"><a href="#资源消耗-2" class="headerlink" title="资源消耗"></a>资源消耗</h4><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2>]]></content>
    
    
    
    <tags>
      
      <tag>Observability</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>MIT6.824-LC2-Note</title>
    <link href="/2023/12/03/note/"/>
    <url>/2023/12/03/note/</url>
    
    <content type="html"><![CDATA[<h2 id="storage-System"><a href="#storage-System" class="headerlink" title="storage System"></a>storage System</h2><ul><li>无状态应用</li><li>有状态应用</li><li>存储系统设计难点<ul><li>高性能</li><li>数据格式</li><li>分布式系统<ul><li>多磁盘</li><li>多网卡</li></ul></li><li>容错设计<ul><li>容错性</li><li>多副本复制</li></ul></li><li>数据同步<ul><li>一致性问题</li><li>一致性协议</li><li>一致性性能</li></ul></li><li>持久化存储</li><li>读写性能提升</li></ul></li></ul><h2 id="GFS"><a href="#GFS" class="headerlink" title="GFS"></a>GFS</h2><ul><li><p>高性能</p><ul><li><p>复制</p></li><li><p>容错</p></li></ul></li><li><p>集群规模(&gt;&#x3D;1000节点)</p></li><li><p>master角色设计</p></li><li><p>不一致性(inconsistencies)</p></li><li><p>磁盘介质的读写性能</p></li><li><p>Big: Large data set</p></li><li><p>Fast: automatic shueding</p></li><li><p>Global: all apps sets same fs</p></li><li><p>FF</p></li><li><p>Design</p></li><li><p>check block</p><ul><li>64MB</li></ul></li><li><p>master</p><ul><li>big+checkpoints</li><li>chunk handle</li><li>version check</li><li>stable storage</li></ul></li></ul><h3 id="文件读取"><a href="#文件读取" class="headerlink" title="文件读取"></a>文件读取</h3><ol><li>客户端向master服务器发送信息: flieName+offset</li><li>master返回信息给客户端: junk handle+that chunk server info+version</li><li>客户端缓存这些信息</li><li>客户端读取离自己最近服务器上的信息</li><li>master服务器检查版本号(version)是否有问题，如果没有问题则发送数据。</li></ol><h3 id="文件写入"><a href="#文件写入" class="headerlink" title="文件写入"></a>文件写入</h3><h4 id="Secondary"><a href="#Secondary" class="headerlink" title="Secondary"></a>Secondary</h4><h4 id="Primary"><a href="#Primary" class="headerlink" title="Primary"></a>Primary</h4><h3 id="append"><a href="#append" class="headerlink" title="append"></a>append</h3><h2 id="consistency"><a href="#consistency" class="headerlink" title="consistency"></a>consistency</h2><h3 id="Ideal-consistency-理想一致性"><a href="#Ideal-consistency-理想一致性" class="headerlink" title="Ideal consistency(理想一致性)"></a>Ideal consistency(理想一致性)</h3><ul><li>分布式系统从体验上来说像是一台机器</li><li></li></ul><h3 id="一致性保证"><a href="#一致性保证" class="headerlink" title="一致性保证"></a>一致性保证</h3><ul><li>Lock</li></ul><h3 id="bad-replication"><a href="#bad-replication" class="headerlink" title="bad replication"></a>bad replication</h3><h3 id="脑裂-split-brain"><a href="#脑裂-split-brain" class="headerlink" title="脑裂(split-brain)"></a>脑裂(split-brain)</h3><h3 id="强一致性-storage-consistency"><a href="#强一致性-storage-consistency" class="headerlink" title="强一致性(storage  consistency)"></a>强一致性(storage  consistency)</h3><h2 id="扩展-Spanner"><a href="#扩展-Spanner" class="headerlink" title="扩展: Spanner"></a>扩展: Spanner</h2><h2 id="思考"><a href="#思考" class="headerlink" title="思考"></a>思考</h2>]]></content>
    
    
    
    <tags>
      
      <tag>distributed system</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>从零实现一个exporter(一)-redis-exporter实现</title>
    <link href="/2023/11/25/%E4%BB%8E%E9%9B%B6%E5%AE%9E%E7%8E%B0%E4%B8%80%E4%B8%AAexporter-%E4%B8%80-redis-exporter%E5%AE%9E%E7%8E%B0/"/>
    <url>/2023/11/25/%E4%BB%8E%E9%9B%B6%E5%AE%9E%E7%8E%B0%E4%B8%80%E4%B8%AAexporter-%E4%B8%80-redis-exporter%E5%AE%9E%E7%8E%B0/</url>
    
    <content type="html"><![CDATA[<h2 id="Redis-Exporter实现"><a href="#Redis-Exporter实现" class="headerlink" title="Redis Exporter实现"></a>Redis Exporter实现</h2><p>根据之前的知识，这里以redis举例来实现一个exporter并且最终通过grafana来展示。</p><h2 id="Redis-Metrics"><a href="#Redis-Metrics" class="headerlink" title="Redis Metrics"></a>Redis Metrics</h2><p>编写代码之前，先选好一些Metrics指标，首先需要明确一点的是指标不是越多越好，因为在关键时刻，像排错等场景。看到那么多指标会无法下手，所以要根据具体场景来挑选，可以先选出一些通用的指标，以及redis在场景中扮演什么样的角色进行挑选。并不是挑选好之后就不用管了，我们还需要进行一些调整。这里挑选出服务端和客户端指标。</p><h3 id="监控方法论"><a href="#监控方法论" class="headerlink" title="监控方法论"></a>监控方法论</h3><ul><li>USE</li><li>黄金指标</li></ul><h3 id="通用指标"><a href="#通用指标" class="headerlink" title="通用指标"></a>通用指标</h3><ul><li>P90</li><li>P95</li><li>P99</li></ul><h3 id="场景指标"><a href="#场景指标" class="headerlink" title="场景指标"></a>场景指标</h3><p>考虑redis作为缓存场景来使用。</p><h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><ul><li><a href="https://cloud.google.com/memorystore/docs/redis/monitoring-metrics?hl=zh-cn">https://cloud.google.com/memorystore/docs/redis/monitoring-metrics?hl=zh-cn</a></li><li><a href="https://help.aliyun.com/zh/redis/user-guide/view-monitoring-data?spm=a2c4g.11186623.0.i7">https://help.aliyun.com/zh/redis/user-guide/view-monitoring-data?spm=a2c4g.11186623.0.i7</a></li><li><a href="https://cloud.tencent.com/document/product/239/48574#redis-.E8.8A.82.E7.82.B9.E7.9B.91.E6.8E.A7">https://cloud.tencent.com/document/product/239/48574#redis-.E8.8A.82.E7.82.B9.E7.9B.91.E6.8E.A7</a></li><li><a href="https://doc.yun.ccb.com/tcloud/Oss/CloudMonitoring/CloudMonitorIndex/RedisMonitorIndexList">https://doc.yun.ccb.com/tcloud/Oss/CloudMonitoring/CloudMonitorIndex/RedisMonitorIndexList</a></li></ul>]]></content>
    
    
    
    <tags>
      
      <tag>Observability</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>监控方法论(一): Google黄金指标与RED方法和USE方法</title>
    <link href="/2023/11/25/%E7%9B%91%E6%8E%A7%E6%96%B9%E6%B3%95%E8%AE%BA-%E4%B8%80-Google%E9%BB%84%E9%87%91%E6%8C%87%E6%A0%87%E4%B8%8ERED%E6%96%B9%E6%B3%95%E5%92%8CUSE%E6%96%B9%E6%B3%95/"/>
    <url>/2023/11/25/%E7%9B%91%E6%8E%A7%E6%96%B9%E6%B3%95%E8%AE%BA-%E4%B8%80-Google%E9%BB%84%E9%87%91%E6%8C%87%E6%A0%87%E4%B8%8ERED%E6%96%B9%E6%B3%95%E5%92%8CUSE%E6%96%B9%E6%B3%95/</url>
    
    <content type="html"><![CDATA[]]></content>
    
    
    
    <tags>
      
      <tag>Observability</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>从零实现一个exporter(零)-exporter基本概念</title>
    <link href="/2023/11/25/%E4%BB%8E%E9%9B%B6%E5%AE%9E%E7%8E%B0%E4%B8%80%E4%B8%AAexporter-%E9%9B%B6-exporter%E5%9F%BA%E6%9C%AC%E6%A6%82%E5%BF%B5/"/>
    <url>/2023/11/25/%E4%BB%8E%E9%9B%B6%E5%AE%9E%E7%8E%B0%E4%B8%80%E4%B8%AAexporter-%E9%9B%B6-exporter%E5%9F%BA%E6%9C%AC%E6%A6%82%E5%BF%B5/</url>
    
    <content type="html"><![CDATA[<h2 id="Exporter简介"><a href="#Exporter简介" class="headerlink" title="Exporter简介"></a>Exporter简介</h2><p>云原生模式下的主流监控已经变成Prometheus为主的一套监控技术栈，下图是Prometheus架构:</p><p><img src="https://prometheus.io/assets/architecture.png" alt="Prometheus architecture"></p><p>而Exporter类似于Agent，它负责收集指定指标，它可以独立出来单独运行，也可以和程序集合在一起。打个比方，之前的中间件，比如Redis，Kafka，ES这些都没有集成Exporter，而在云原生中，就需要通过operator去集成。而一些比较新的中间件，比如pulsar，ClickHouse等等则已经集成了，不需要去创建。</p><p>通过架构图可以看出来，Exporter收集到信息之后并不是主动pull到Prometheus上，而是创建一个内部http服务器，暴露&#x2F;metrics端点，由Prometheus定期去轮询这些Exporter，之后存放在Prometheus中。</p><h3 id="Metrics"><a href="#Metrics" class="headerlink" title="Metrics"></a>Metrics</h3><p>具体写代码之前，我们需要明白什么是Metics，也就是指标。Metics是一个二元组:&lt;value: timestamp&gt;。表示这个指标在某个时间段的值是多少，举个例子，一个图片压缩服务，它的指标如下所示:</p><ol><li><strong>处理速度（Processing Speed）：</strong><ul><li><strong>指标：</strong> 每秒处理的图片数量、平均处理时间等。</li><li><strong>原因：</strong> 了解服务的处理能力，确保它满足预期的性能需求。</li></ul></li><li><strong>资源利用率（Resource Utilization）：</strong><ul><li><strong>指标：</strong> CPU 使用率、内存使用率、磁盘空间使用率等。</li><li><strong>原因：</strong> 监测服务的资源消耗，预防潜在的性能瓶颈和资源耗尽问题。</li></ul></li><li><strong>请求成功率（Success Rate）：</strong><ul><li><strong>指标：</strong> 成功处理的请求比例、失败请求比例等。</li><li><strong>原因：</strong> 确保服务按预期处理请求，并及时检测和解决任何处理失败的问题。</li></ul></li><li><strong>请求队列长度（Request Queue Length）：</strong><ul><li><strong>指标：</strong> 待处理请求的队列长度。</li><li><strong>原因：</strong> 监控请求队列长度，避免请求积压导致性能下降。</li></ul></li><li><strong>错误率（Error Rate）：</strong><ul><li><strong>指标：</strong> 处理中发生错误的请求比例。</li><li><strong>原因：</strong> 跟踪服务的错误率，及时发现并解决潜在问题。</li></ul></li><li><strong>网络延迟（Network Latency）：</strong><ul><li><strong>指标：</strong> 请求的网络往返时间（Round-Trip Time，RTT）。</li><li><strong>原因：</strong> 确保网络延迟在可接受范围内，避免用户体验差。</li></ul></li><li><strong>系统负载（System Load）：</strong><ul><li><strong>指标：</strong> 系统的平均负载。</li><li><strong>原因：</strong> 监控系统的负载，预防过载导致的性能下降。</li></ul></li><li><strong>缓存命中率（Cache Hit Rate）：</strong><ul><li><strong>指标：</strong> 图片压缩服务的缓存命中率。</li><li><strong>原因：</strong> 了解缓存的效果，提高性能和降低对底层存储系统的依赖。</li></ul></li><li><strong>服务可用性（Service Availability）：</strong><ul><li><strong>指标：</strong> 服务的可用性百分比。</li><li><strong>原因：</strong> 确保服务随时可用，通过监控可用性识别潜在的故障。</li></ul></li><li><strong>服务响应时间（Service Response Time）：</strong><ul><li><strong>指标：</strong> 请求到达服务并获得响应的时间。</li><li><strong>原因：</strong> 监测服务的响应时间，确保它在用户期望的范围内。</li></ul></li></ol><h3 id="Metrics-type"><a href="#Metrics-type" class="headerlink" title="Metrics  type"></a>Metrics  type</h3><p>了解完Metrics之后，需要了解下关于Metrics Type，一共有四种核心类型:</p><ul><li><a href="https://prometheus.io/docs/concepts/metric_types/#counter">Counter </a>: counter是一个累计指标，代表一个指标只会增不会减，它只有在重启和重置才会为零。可以使用它来表示已经处理过的请求数量，已经完成的任务数量，或者错误数量。</li><li><a href="https://prometheus.io/docs/concepts/metric_types/#gauge">Gauge </a>: Gauge是一个代表单个数值的指标，可以任意上升或下降。比如当前内存使用量或者进程数量，以及并发请求数量。</li><li><a href="https://prometheus.io/docs/concepts/metric_types/#histogram">Histogram </a> histogram对观测数量(通常是请求持续时间或响应大小等)进行采样，并将其计数在可配置桶中，它还提供了所有观测值的综合。</li><li><a href="https://prometheus.io/docs/concepts/metric_types/#summary">Summary </a> : 有点类似于Historygram，summary观测值一般是请求持续时间和响应大小之类的，并且提供观测值总和还有所有观测值总数。可以用来表示P99&#x2F;P95等等。</li></ul><h3 id="Prometheus-client"><a href="#Prometheus-client" class="headerlink" title="Prometheus client"></a>Prometheus client</h3><p>client暂时没什么好说的，之前提到的metrics相关操作支持比较好的客户端是:</p><ul><li>Go</li><li>Java</li><li>Python</li><li>Ruby</li><li>.Net</li></ul><h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><ul><li><a href="https://prometheus.io/docs/instrumenting/writing_exporters/#writing-exporters">https://prometheus.io/docs/instrumenting/writing_exporters/#writing-exporters</a></li></ul>]]></content>
    
    
    
    <tags>
      
      <tag>Observability</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Linux存储学习(一)基本知识</title>
    <link href="/2023/10/22/Linux%E5%AD%98%E5%82%A8%E5%AD%A6%E4%B9%A0-%E4%B8%80-%E5%9F%BA%E6%9C%AC%E7%9F%A5%E8%AF%86/"/>
    <url>/2023/10/22/Linux%E5%AD%98%E5%82%A8%E5%AD%A6%E4%B9%A0-%E4%B8%80-%E5%9F%BA%E6%9C%AC%E7%9F%A5%E8%AF%86/</url>
    
    <content type="html"><![CDATA[<h2 id="背景"><a href="#背景" class="headerlink" title="背景"></a>背景</h2><p>对Linux存储方面的知识比较杂，没有整理相关知识，相关笔记是为了记录这一块的知识。</p><h2 id="存储协议"><a href="#存储协议" class="headerlink" title="存储协议"></a>存储协议</h2><h3 id="PCIe"><a href="#PCIe" class="headerlink" title="PCIe"></a>PCIe</h3><h3 id="SATA-Serial-Advanced-Technology-Attachment"><a href="#SATA-Serial-Advanced-Technology-Attachment" class="headerlink" title="SATA(Serial Advanced Technology Attachment)"></a>SATA(Serial Advanced Technology Attachment)</h3><h3 id="SAS-Serial-Attached-SCSI"><a href="#SAS-Serial-Attached-SCSI" class="headerlink" title="SAS(Serial Attached  SCSI)"></a>SAS(Serial Attached  SCSI)</h3><h3 id="AHCI-Advanced-Host-Controller-Interface"><a href="#AHCI-Advanced-Host-Controller-Interface" class="headerlink" title="AHCI(Advanced Host Controller Interface)"></a>AHCI(Advanced Host Controller Interface)</h3><h2 id="存储介质"><a href="#存储介质" class="headerlink" title="存储介质"></a>存储介质</h2><h3 id="HHD"><a href="#HHD" class="headerlink" title="HHD"></a>HHD</h3><h3 id="SSD"><a href="#SSD" class="headerlink" title="SSD"></a>SSD</h3><h3 id="NVMe"><a href="#NVMe" class="headerlink" title="NVMe"></a>NVMe</h3><h2 id="存储类型"><a href="#存储类型" class="headerlink" title="存储类型"></a>存储类型</h2><h3 id="对象存储"><a href="#对象存储" class="headerlink" title="对象存储"></a>对象存储</h3><h3 id="文件存储"><a href="#文件存储" class="headerlink" title="文件存储"></a>文件存储</h3><h3 id="块存储"><a href="#块存储" class="headerlink" title="块存储"></a>块存储</h3><h2 id="本地存储"><a href="#本地存储" class="headerlink" title="本地存储"></a>本地存储</h2><p>在Linux操作系统中，对于调用存储相关API的应用程序来说，所有设备都是以文件形式使用的。比如说网络程序使用socket打开一个套接字，返回结果是一个文件描述符。</p><h2 id="网络存储"><a href="#网络存储" class="headerlink" title="网络存储"></a>网络存储</h2><h2 id="虚拟文件系统-Virtual-File-System-VFS"><a href="#虚拟文件系统-Virtual-File-System-VFS" class="headerlink" title="虚拟文件系统(Virtual File System,VFS)"></a>虚拟文件系统(Virtual File System,VFS)</h2><h2 id="文件系统"><a href="#文件系统" class="headerlink" title="文件系统"></a>文件系统</h2><h3 id="ext2-3-4"><a href="#ext2-3-4" class="headerlink" title="ext2&#x2F;3&#x2F;4"></a>ext2&#x2F;3&#x2F;4</h3><h3 id="zfs"><a href="#zfs" class="headerlink" title="zfs"></a>zfs</h3><h2 id="存储评价指标"><a href="#存储评价指标" class="headerlink" title="存储评价指标"></a>存储评价指标</h2><h3 id="性能指标"><a href="#性能指标" class="headerlink" title="性能指标"></a>性能指标</h3><p>业界有相关存储性能委员会(Storage Performance Council,SPC)发布相应测试规范和相关测试工作集。典型的测试工作集包括以下内容:</p><ul><li>SPC-1: 主要评估存储系统面向事务性业务的性能。</li><li>SPC-2: 评估不同业务类型，大规模连续数据访问的存储系统的性能，如大量文件并发性访问、视频点播业务等。</li><li>SPC-3: 提供应用层的模拟，如存储管理、内容管理、信息生命周期等性能。</li></ul><h3 id="可靠性标准"><a href="#可靠性标准" class="headerlink" title="可靠性标准"></a>可靠性标准</h3><p>在不同领域有不同标准，但是以下内容是通用性标准:</p><ol><li>数据可用性</li><li>数据完整性</li><li>数据安全性</li></ol><h3 id="功能性标准"><a href="#功能性标准" class="headerlink" title="功能性标准"></a>功能性标准</h3><p>如是否符合业界定义的规范，不同的国家有不同的标准。这一块可以参考类似于EMC、华为、NetApp提供的产品。</p><h3 id="能耗标准"><a href="#能耗标准" class="headerlink" title="能耗标准"></a>能耗标准</h3><p>主要用来评估存储系统在不同负载情况下的消耗，如果在相同负载下，功耗消耗越小的越有竞争力。因为这个标准涉及到运维的代价以及成本代价。所以数据中心的运行者对这个指标比较关心。</p>]]></content>
    
    
    
    <tags>
      
      <tag>storage</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>homeLab搭建全过程</title>
    <link href="/2023/10/21/homeLab%E6%90%AD%E5%BB%BA%E5%85%A8%E8%BF%87%E7%A8%8B/"/>
    <url>/2023/10/21/homeLab%E6%90%AD%E5%BB%BA%E5%85%A8%E8%BF%87%E7%A8%8B/</url>
    
    <content type="html"><![CDATA[<h2 id="背景"><a href="#背景" class="headerlink" title="背景"></a>背景</h2><p>前段时间把手上的MBP2015给换掉了，这个时候就空出一台mini PC。之前一直想尝试PVE或者ESXI。但是尝试了几次发现上手比较困难，关键问题还是对Linux掌握的不够熟悉。其实主要就是WiFi比较麻烦，如果搞定了网络连接就非常简单了。</p><h2 id="配置"><a href="#配置" class="headerlink" title="配置"></a>配置</h2><ul><li>宿主机操作系统: win11</li><li>CPU AMD zen3 锐龙5000 8核16线程</li><li>内存条 三星DDR4 3200 32*2</li><li>硬盘<ul><li>SSD 三星980 pro 2T</li><li>HDD 西部数据 2T</li></ul></li></ul><p>主要是跑K8S集群和备份数据的作用。</p><h2 id="集群配置"><a href="#集群配置" class="headerlink" title="集群配置"></a>集群配置</h2><p>虚拟化使用了VMware，创建虚拟机非常的简单，包括后续弄几台机器也是一样的，就是比较麻烦，因为使用DHCP每次IP都会发生变化，而K8S apiserver如果发生了IP变化就会导致etcd无法和apiserver进行交互。所以就需要设置静态IP，这样就会非常麻烦了，为了解决这个问题，想到了批量装机的方法。这里简单回忆一下，过去的有一段时间了。</p><p>首先将VMware中的网络模式设置为NAT模式:</p><p><img src="/../public/img/homeLab/natpng.png" alt="NAT设置"></p><p>之后需要修改VMware8网卡，设置好IP地址，子网掩码等相关配置。</p><p><strong>TODO相关图片</strong></p><p>修改完成之后来到虚拟机中，修改虚拟机配置;</p><p><strong>TODO相关图片</strong></p><h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2>]]></content>
    
    
    
    <tags>
      
      <tag>homeLab</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>MapReduce论文笔记</title>
    <link href="/2023/10/16/MapReduce%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/"/>
    <url>/2023/10/16/MapReduce%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/</url>
    
    <content type="html"><![CDATA[<h2 id="背景"><a href="#背景" class="headerlink" title="背景"></a>背景</h2><p>在学MIT 6.824的时候，它的第一个Lab要求实现一个单机并发版本的MapReduce，所以这篇文章主要记录关于MapReduce和实验对应的内容。</p><h2 id="Programming-Model"><a href="#Programming-Model" class="headerlink" title="Programming Model"></a>Programming Model</h2><p>这里主要介绍了MapReduce的编程模型，它会接收一组键值对作为输入任务，同时输出一对键值对作为结果。它提供了两个函数给用户使用, 需要注意的是Map和Reduce都需要用户自己来实现:</p><ul><li>Map: 接收一对键值对，如何生成相关一组值。将所有相同中间键的中间值组合在一起，之后传输给Reduce函数。</li><li>Reduce: 接受一个中间键和与该键相关的一组值, 将这些值合并在一起，形成一个可能更小的值集合。通常，每次调用Reduce函数只会生成零个或一个输出值。中间值通过一个迭代器(iterator)传递给用户的Reduce函数。这允许我们处理那些太大以至于无法在内存中存储的值列表。</li></ul><p>具体来说，Map函数负责将输入数据映射为中间键&#x2F;值对，这些中间键&#x2F;值对将按照键的相同性被分组，然后传递给Reduce函数。Reduce函数则负责处理这些分组后的中间值，将它们合并在一起，生成最终的输出键&#x2F;值对。</p><p>这种分离的处理方式允许MapReduce框架高效地处理大规模数据，并且让用户专注于编写特定的处理逻辑，而不需要担心底层的数据分发和整合。</p><h3 id="经典的例子-单词统计-world-count"><a href="#经典的例子-单词统计-world-count" class="headerlink" title="经典的例子-单词统计(world count)"></a>经典的例子-单词统计(world count)</h3><p><strong>Map函数：</strong></p><ul><li>输入：一个文档的名称（key）和该文档的内容（value）。</li><li>处理：Map函数遍历文档的内容，将文档内容分割成单词。对于每个单词，Map函数发出中间键&#x2F;值对，其中键是单词（word），值是固定的字符串“1”（表示该单词出现了一次）。</li><li>输出：多个中间键&#x2F;值对，例如（”apple”, “1”），（”orange”, “1”），等等。</li></ul><p><strong>Reduce函数：</strong></p><ul><li>输入：一个单词（key）和该单词的计数列表（values）。</li><li>处理：Reduce函数迭代计数列表，将所有计数相加，得到该单词的总出现次数。</li><li>输出：一个键&#x2F;值对，其中键是单词，值是该单词的总出现次数，转换为字符串形式。</li></ul><h3 id="更多例子"><a href="#更多例子" class="headerlink" title="更多例子"></a>更多例子</h3><h4 id="分布式Grep（分布式文本搜索）"><a href="#分布式Grep（分布式文本搜索）" class="headerlink" title="分布式Grep（分布式文本搜索）"></a>分布式Grep（分布式文本搜索）</h4><h4 id="统计URL访问频率"><a href="#统计URL访问频率" class="headerlink" title="统计URL访问频率"></a>统计URL访问频率</h4><h4 id="反向Web链接图"><a href="#反向Web链接图" class="headerlink" title="反向Web链接图"></a>反向Web链接图</h4><h4 id="每个主机的词项向量（文本摘要）"><a href="#每个主机的词项向量（文本摘要）" class="headerlink" title="每个主机的词项向量（文本摘要）"></a>每个主机的词项向量（文本摘要）</h4><h2 id="MapReduce工作流程"><a href="#MapReduce工作流程" class="headerlink" title="MapReduce工作流程"></a>MapReduce工作流程</h2><p><img src="https://pic1.zhimg.com/80/v2-5642ac8d1e37098be1c97836341a7210_720w.webp" alt="img"></p><p>用户程序中的MapReduce库会先将输入文件切分为M个片段，通常每个片段在16MB到64MB之间，具体大小可以让用户通过参数进行指定。之后创建多个程序副本。</p><p>副本角色分为两种: master和worker，集群中只有一个master副本，剩下都是worker副本。master会对worker进行任务分配，这里有M个Map任务以及R个Reduce任务要进行分配。master会给每个空闲的worker分配一个map任务或者一个reduce任务。</p><p>被分配到map任务的worker会读取相关输入数据片段，之后会从数据数据中解析出键值对，并将它们传入到用户自定义Map函数中。Map函数所生成的中间键值对会被缓存在内存中。</p><p>每隔一段时间，被缓存的键值对会被写入到本地硬盘，并通过分区函数分到R个区域内。这些被缓存的键值对在本地磁盘的位置会被传回master。master负责将这些位置转发给执行reduce操作的worker。</p><p>当master将这些位置告诉了某个执行reduce的worker，该worker就会使用RPC的方式去从保存了这些缓存数据的map worker的本地磁盘中读取数据。当一个reduce worker读取完了所有的中间数据后，它就会根据中间键进行排序，这样使得具有相同键值的数据可以聚合在一起。之所以需要排序是因为通常许多不同的key会映射到同一个reduce任务中。如果中间数据的数量太过庞大而无法放在内存中，那就需要使用外部排序。</p><p>reduce worker会对排序后的中间数据进行遍历。然后，对于遇到的每个唯一的中间键，reduce worker会将该key和对应的中间value的集合传入用户所提供的Reduce函数中。Reduce函数生成的输出会被追加到这个reduce分区的输出文件中。</p><p>当所有的map任务和reduce任务完成后，master会唤醒用户程序。此时，用户程序会结束对MapReduce的调用。</p>]]></content>
    
    
    
    <tags>
      
      <tag>distributed system</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>MIT6.824-Lab1-MapReduce</title>
    <link href="/2023/10/16/MIT6-824-Lab1-MapReduce/"/>
    <url>/2023/10/16/MIT6-824-Lab1-MapReduce/</url>
    
    <content type="html"><![CDATA[<h2 id="背景"><a href="#背景" class="headerlink" title="背景"></a>背景</h2><p>这个Lab主要是参考了MapReduce这篇论文实现的，所以只需要读懂了MapReduce中2-3章节就可以了。我们需要实现worker，他主要有两个任务分别是Map和Reduce，来分别读取和写入文件。同时还有一个coordinator ，它有两个任务: 分配任务给worker和处理失败任务。Lab还给出了一个参考代码:mrsequential.go。</p><h2 id="任务"><a href="#任务" class="headerlink" title="任务"></a>任务</h2><p>Lab要求我们实现一个分布式MapReduce，在之前背景中也介绍了:</p><ul><li>coordinator</li><li>worker</li></ul><p>我们只需要实现一个coordinator就好，但是有一个或多个并行运行的worker。在真实的分布式系统中，worker都是分布在不同的机器上面。但是在这个Lab中所有的程序都运行在一台机器上。worker想要通过远程调用(RPC)和coordinator进行通信。每个worker都会向coordinator请求任务从一个或多个文件中读取任务的输入数据，执行任务，然后将任务的输出写入一个或多个文件。如果一个worker没有在合理时间内完成任务（在这个Lab中，规定为十秒），coordinator应该能够察觉到，并将同样的任务分配给另一个worker。</p><p>我们需要修改以下代码来完成实验:</p><ul><li>mr&#x2F;coordinator.go</li><li>mr&#x2F;worker.go</li><li>mr&#x2F;rpc.go</li></ul><h2 id="验证"><a href="#验证" class="headerlink" title="验证"></a>验证</h2><p>完成代码之后，我们需要验证是否正确。首先需要构建word-count插件:</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs shell"><span class="hljs-meta prompt_">$ </span><span class="language-bash">go build -buildmode=plugin ../mrapps/wc.go</span><br></code></pre></td></tr></table></figure><p>在main文件夹中，运行coordinator</p><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs sh">$ <span class="hljs-built_in">rm</span> mr-out*<br>$ go run mrcoordinator.go pg-*.txt<br></code></pre></td></tr></table></figure><p>这里的<code>pg-*.txt</code>参数是输入文件；每个文件对应一个“split”，是一个Map任务的输入。</p><p>接着在一个新的窗口或者打开多个窗口来运行worker程序:</p><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs sh">$ go run mrworker.go wc.so<br></code></pre></td></tr></table></figure><p>当workers和coordinator完成的时候，查看输出文件mr-out-*，Lab完成的时候，输出文件排序应该是有序的，类似于这样:</p><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs sh">$ <span class="hljs-built_in">cat</span> mr-out-* | <span class="hljs-built_in">sort</span> | more<br>A 509<br>ABOUT 2<br>ACT 8<br>...<br></code></pre></td></tr></table></figure><p>最后运行测试脚本:</p><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs sh">$ <span class="hljs-built_in">cd</span> ~/6.5840/src/main<br>$ bash test-mr.sh<br>*** Starting <span class="hljs-built_in">wc</span> <span class="hljs-built_in">test</span>.<br></code></pre></td></tr></table></figure><p>在coordinator无法正常结束的情况下，测试脚本会一直挂起。你可以在<code>mr/coordinator.go</code>文件的<code>Done</code>函数中将<code>ret := false</code>改为<code>ret := true</code>，这样协调器会立即退出。然后再次运行测试脚本</p><p>当完成之后，输出结果如下就是表示测试通过了:</p><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><code class="hljs sh">$ bash test-mr.sh<br>*** Starting <span class="hljs-built_in">wc</span> <span class="hljs-built_in">test</span>.<br>--- <span class="hljs-built_in">wc</span> <span class="hljs-built_in">test</span>: PASS<br>*** Starting indexer <span class="hljs-built_in">test</span>.<br>--- indexer <span class="hljs-built_in">test</span>: PASS<br>*** Starting map parallelism <span class="hljs-built_in">test</span>.<br>--- map parallelism <span class="hljs-built_in">test</span>: PASS<br>*** Starting reduce parallelism <span class="hljs-built_in">test</span>.<br>--- reduce parallelism <span class="hljs-built_in">test</span>: PASS<br>*** Starting job count <span class="hljs-built_in">test</span>.<br>--- job count <span class="hljs-built_in">test</span>: PASS<br>*** Starting early <span class="hljs-built_in">exit</span> <span class="hljs-built_in">test</span>.<br>--- early <span class="hljs-built_in">exit</span> <span class="hljs-built_in">test</span>: PASS<br>*** Starting crash <span class="hljs-built_in">test</span>.<br>--- crash <span class="hljs-built_in">test</span>: PASS<br>*** PASSED ALL TESTS<br>$<br></code></pre></td></tr></table></figure><h2 id="实现思路"><a href="#实现思路" class="headerlink" title="实现思路"></a>实现思路</h2><h3 id="worker"><a href="#worker" class="headerlink" title="worker"></a>worker</h3><h3 id="master"><a href="#master" class="headerlink" title="master"></a>master</h3><h3 id="rpc"><a href="#rpc" class="headerlink" title="rpc"></a>rpc</h3><h2 id="思考"><a href="#思考" class="headerlink" title="思考"></a>思考</h2><ul><li>master如何去分配任务类型？比如是根据什么原因给某个worker分配map或者reduce任务的？</li></ul>]]></content>
    
    
    
    <tags>
      
      <tag>distributed system</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>redis-proxy初探</title>
    <link href="/2023/10/07/redis-proxy%E5%88%9D%E6%8E%A2/"/>
    <url>/2023/10/07/redis-proxy%E5%88%9D%E6%8E%A2/</url>
    
    <content type="html"><![CDATA[]]></content>
    
    
    
    <tags>
      
      <tag>redis</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>redis集群模式</title>
    <link href="/2023/10/07/redis%E9%9B%86%E7%BE%A4%E6%A8%A1%E5%BC%8F/"/>
    <url>/2023/10/07/redis%E9%9B%86%E7%BE%A4%E6%A8%A1%E5%BC%8F/</url>
    
    <content type="html"><![CDATA[<h2 id="redis集群"><a href="#redis集群" class="headerlink" title="redis集群"></a>redis集群</h2><p>redis集群是为了解决redis单机上线而设计出来的，</p><h2 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h2>]]></content>
    
    
    
    <tags>
      
      <tag>redis</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>SRE理解</title>
    <link href="/2023/09/28/SRE%E7%90%86%E8%A7%A3/"/>
    <url>/2023/09/28/SRE%E7%90%86%E8%A7%A3/</url>
    
    <content type="html"><![CDATA[<h2 id="背景"><a href="#背景" class="headerlink" title="背景"></a>背景</h2><p>之前有一段时间做过半个月的SRE，当时并没有完全理解这个岗位是做什么的。后面兜兜转转又做回了SRE，这里总结下工作几个月来对SRE的理解，以及一些工作内容，同时还参考了别人对SRE的介绍。</p><h2 id="SRE定义"><a href="#SRE定义" class="headerlink" title="SRE定义"></a>SRE定义</h2><p>SRE(site reliability engineer)在各种社区上被说成是运维，是背锅的。或者干脆说SRE是devops是运维这种；这里简单说下我对SRE的理解，SRE主要是负责三个核心内容:</p><ul><li>稳定</li><li>成本</li><li>效率</li></ul><p>首先来说下稳定性，这里的稳定性更多的是关于SLO一块为主。SLO也就是常说的几个9，比如某服务在一年的时间里面是4个9，那为了实现这个目标，就需要做多活和容灾等等。这里就不展开细说，可以留在下次水下，总的来说SRE在稳定性方面的工作就是围绕着SLO展开。也包括了日常值班和节假期值班。</p><p>成本方面涉及到多方面，比如说FinOPS思想，如何用更少的机器支持更多的服务？或者如何让资源得到最大的利用？又或者说之前提高的SLO，假设现在4个9可以满足，但是被强行要求上5个9，那付出的人力成本和机器成本也就越多。像成本一块考虑的也会多得多。</p><p>最后来说下效率，效率这边和devops有些重复，可能有时候做了devops相关的事情。比如说CICD流水线这种，也有在运维中过程中去自动化一块重复的事情，解决一些琐事。比如说集群巡检，日常清理等等。</p><p>SRE总体来说是一个非常具有挑战性的岗位，但是很多人对这个各位的误解也很多，认为就是运维，做的内容都是一些低级重复的工作。其实SRE是需要运维经验丰富的软件开发工程师或者具有开发能力的运维工程师。</p><p>当然不同的公司对SRE定义也不一样，这里借用laixintao大佬博客中提到各家公司对SRE的定位:</p><blockquote><p>比如蚂蚁金服有两种 SRE，一种是负责稳定性的，就是大家所理解的 SRE；另一种叫做资金安全 SRE，并不负责服务正常运行，而是负责金钱数目正确，对账没有错误，工作内容以开发为主，主要是资金核对平台和核对规则（没有做过，只是个人理解）。某种意义上说，已经不算是 SRE 而是专业领域的开发了。</p><p><a href="https://www.youtube.com/watch?v=koGaH4ffXaU">Netflix</a> （2016年）的模式是谁开发，谁维护。SRE 负责提供技术支持，和咨询服务。Netflix 在全球 170 个国家有服务，Core SREs 只有 5 个人。</p><p>微软有专门的 [Game Streaming SRE](<a href="https://azure.microsoft.com/mediahandler/files/resourcefiles/devops-at-microsoft-game-streaming-sre/DevOps">https://azure.microsoft.com/mediahandler/files/resourcefiles/devops-at-microsoft-game-streaming-sre/DevOps</a> at Microsoft - Xbox game streaming SRE.pdf)，负责 XBox 在线游戏的稳定性。</p></blockquote><p>所以不同公司对SRE工作内容是不一样，取决于这家公司性质是什么的。比如我当前所做的主要是保证开发集群，测试集群，验收集群的K8S以及中间件稳定性。相对生产集群来说SLO要求不会太高，面向的也是开发和测试人员。</p><p>在工作过程中可以接触到新的知识和新的项目，也可以造轮子，会比较有意思。以成本和效率为主。这里并没有说稳定性不重要，而是相对生产集群来说会要求的轻松一点。而生产集群第一位就是稳定性，之后才会去考虑其他东西。相对来说会比较无聊。这里只是个人体验。</p><p>这里可以将SRE简单分几类:</p><blockquote><ol><li>Infrastructure：主要负责最基础的硬件设施，网络，类似于 IaaS，做的事情可参考 DigitalOcean</li><li>Platform：提供中间件技术，开箱即用的一些服务，类似于 PaaS，做的事情可参考 Heroku, GCP, AWS 等</li><li>业务 SRE：维护服务，应用，维护业务的正常运行</li></ol></blockquote><p>我应该是偏向于Platform和业务相关。</p><h2 id="SRE工作内容"><a href="#SRE工作内容" class="headerlink" title="SRE工作内容"></a>SRE工作内容</h2><p>根据之前在SRE中的定义，这里也将三类工作内容简单来进行一个描述，同时配合一些公司的招聘来理解。</p><h3 id="Infrastructure-SRE"><a href="#Infrastructure-SRE" class="headerlink" title="Infrastructure SRE"></a>Infrastructure SRE</h3><p>依旧是大佬博客中他对Infrastructure SRE的理解，不过前提是需要自建data center(DC)才会需要Infrastructure SRE。</p><blockquote><ol><li>负责服务器的采购，预算，CMDB 管理。要知道（能查询到）每一台的负责人是谁，在干什么。这个非常重要，如果做不好，会造成极大的资源浪费。</li><li>提供可靠软件的部署环境，一般是虚拟机，或者 bare mental。</li><li>操作系统的版本统一维护，Linux 发行版的版本，Kernel 的版本等。</li><li>维护机器上的基础软件，比如 NTP，监控代理，其他的一些代理。</li><li>提供机器的登录方式，权限管理，命令审计。</li><li>维护一套可观测性的基础设施，比如监控系统，log 系统，trace 系统。</li><li>维护网络，大公司可能都会自己设计机房内的网络。其中包括：<ol><li>网络的连通，这个是必要的。对于上层用户（Platform SRE）来说，交付的服务应该是任意两个 IP 是可以 ping 通的，即管理好 3 层以下的网络。</li><li>NAT 服务</li><li>DNS 服务</li><li>防火墙</li><li>4 层负载均衡，7层负载均衡</li><li>CDN</li><li>证书管理</li></ol></li></ol></blockquote><p>一些对Infrastructure SRE招聘要求:</p><figure class="highlight markdown"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><code class="hljs markdown"><br><span class="hljs-bullet">1.</span> 负责服务器各类场景技术评估、监控、调优、诊断及硬件优化和故障定位分析<br><span class="hljs-bullet">2.</span> 负责服务器生命周期过程技术优化、硬件原理和主要特性、完善技术可用性实践<br><span class="hljs-bullet">3.</span> 评估硬件功能方案、基于新产品的运维场景下、完善各个过程的新产品适配可用维保障<br><span class="hljs-bullet">4.</span> 负责设备生命周期自运营维护；完善运维过程的硬件/系统的技术方案输出和标准化<br><span class="hljs-bullet">5.</span> 熟悉X86平台服务器和主要部件的架构和主要特性、及硬件底层的故障判断和分析能力；<br><br>职位要求<br><span class="hljs-bullet">1.</span> 熟练使用Linux系统，具备Python/shell等脚本语言，部署开发、测试环境 ；<br><span class="hljs-bullet">2.</span> 精通X86服务器硬件组件/子系统CPU，Disk,Memory PSU等验证方案者优先；<br><span class="hljs-bullet">3.</span> 具有较强的分析问题解决问题的能力，具有良好的团队沟通协作能力；<br><span class="hljs-bullet">4.</span> 熟悉自动化运维技术，能充分利用自动化运维来提高工作效率；<br><span class="hljs-bullet">5.</span> 学习能力强，技术兴趣广泛；责任心强，对工作充满热情。<br><span class="hljs-bullet">6.</span> 熟悉服务器厂商售后及机房现场管理。<br></code></pre></td></tr></table></figure><p>可以看到基本上都是以硬件为主。</p><h3 id="Platform-SRE"><a href="#Platform-SRE" class="headerlink" title="Platform SRE"></a>Platform SRE</h3><p>同样的Platform SRE和Infrastructure SRE都有类似的地方，就是如果是购买第三方服务，比如说阿里云，腾讯云，AWS等等。其实就不需要相关SRE了。但是如果是自建的就需要相关SRE来维护和提供稳定性。</p><blockquote><p>Infrastructure SRE 维护的是基础设施，Platform SRE 使用他们提供的基础设施建立软件服务，让公司内的开发者可以使用开箱即用的软件服务，比如 Queue，Cache，定时任务，RPC 服务等等。</p><p>主要的工作内容有：</p><ol><li>RPC 服务：让不同的服务可以互相发现并调用</li><li>私有云服务</li><li>队列服务，比如 Kafka 或者 RabbitMQ</li><li>分布式的 cronjob 服务</li><li>Cache</li><li>网关服务：反向代理的配置</li><li>对象存储：s3</li><li>其他一些数据库：ES，mongo 等等。一般来说，关系型数据库会有 DBA 来运维，但是 NoSQL 或者图数据库一般由 SRE 维护。</li><li>内部的开发环境：</li><li>SCM 系统，比如自建的 Gitlab</li><li>CI&#x2F;CD 系统</li><li>镜像系统，比如 Harbor</li><li>其他的一些开发工具，比如分布式编译，Sentry 错误管理等等</li><li>一些离线计算环境，大数据的服务</li></ol></blockquote><p>招聘要求:</p><figure class="highlight markdown"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><code class="hljs markdown">1、负责接入层在向云原生转型过程中的规划、设计、部署、以及业务性能调优；<br>2、负责接入层管控层面的整体方案设计和推进，结合云原生的容器调度体系（K8S），在业务高稳定性同时，做到docker镜像化，自动化运维，探索研究新的技术方向，例如infra as code，不断提升运维工作效率；<br>3、负责接入层在各项大促（例如双十一）期间的稳定性、规模化以及性能保障，确保峰值时期的平稳运行。<br>4、负责接入层技术支持和日常运维工作，对突发事件的快速响应、定位及处理，排除故障，保障系统稳定性；<br>职位要求<br>1、精通TCP/HTTP(2)/DNS协议原理；<br>2、熟悉golang/C/Java/Python/Shell中的任意一种以上；<br>3、熟悉常见的配置管理和运维工具，如：Ansible、Puppet、SaltStack、Fabric、Kubenetes、Docker等；<br>4、熟悉nginx、lvs、envoy、service mesh等技术，对ngx<span class="hljs-emphasis">_lua有实践者优先</span><br><span class="hljs-emphasis">4、熟悉阿里云ECS、OSS、SLB、CDN等云产品优先；</span><br><span class="hljs-emphasis">5、熟悉云计算平台OpenStack、Kubernetes、Mesos、Swram及docker/kvm/xen等虚拟化技术优先；</span><br><span class="hljs-emphasis">6、热爱技术，自我驱动，主动思考，不断钻研和探索新领域，有较好的技术敏感度、风险识别能力和全局意识；</span><br><span class="hljs-emphasis">7、高度的责任心，良好的沟通能力和团队协作精神，有较强的跨团队协调能力且抗压能力强。</span><br><span class="hljs-emphasis"></span><br><span class="hljs-emphasis">1. 推进基础设施云原生架构演进，如基础设施即代码（IAC）、Serverless、GitOps等；</span><br><span class="hljs-emphasis">2. 标准化调度系统监控，日志采集，包括SLA的制定与故障定位；</span><br><span class="hljs-emphasis">3. 建设自动化及工程化的解决方式，以减少在传统运维层面的人力投入，做到无人值守。</span><br><span class="hljs-emphasis">4. 建设基础设施的高可用技术风险体系，如变更防御、异常定位和自愈系统。</span><br><span class="hljs-emphasis">职位要求</span><br><span class="hljs-emphasis">1. 有强烈的技术热情，工作责任感，有开源社贡献优先；</span><br><span class="hljs-emphasis">2. 至少精通一门编程语言，Golang/Java优先；</span><br><span class="hljs-emphasis">3. 熟悉云原生相关技术，熟练掌握Docker、K8S 等主流云技术，有Terraform使用和研发经验优先；</span><br><span class="hljs-emphasis">4. 熟悉Linux系统和Shell，对网络、存储等基础设施领域有一定的了解和知识储备；</span><br><span class="hljs-emphasis">5. 熟悉运维自动化部署平台研发，具有大规模集群架构设计经验优先；</span><br><span class="hljs-emphasis">6. 有良好的沟通，团队协作能力，熟悉DevOps流程。</span><br></code></pre></td></tr></table></figure><p>可以看到其中提到了关于私有云，也就是如果是自建DC的话，就需要自建去实现一套私有云服务，如果做不错的话，说不定还可以对外提供这种服务。实现营收。</p><h3 id="业务SRE"><a href="#业务SRE" class="headerlink" title="业务SRE"></a>业务SRE</h3><p>这一块我目前并没有接触，所以对于这一段理解的并不深。简单来说就是围绕着业务展开，保障业务在运行;</p><blockquote><p>这一层的 SRE 更加贴近于业务，知道业务是怎么运行的，请求是怎么处理的，依赖了哪些组件。如果 X 除了问题，可以有哪些降级策略。参与应用的架构设计，提供技术支持。</p><p>主要的工作内容有：</p><ol><li>参与系统的设计。比如熔断、降级，扩容等策略。</li><li>做压测，了解系统的容量。</li><li>做容量规划。</li><li>业务侧的 Oncall。</li></ol></blockquote><h2 id="日常工作"><a href="#日常工作" class="headerlink" title="日常工作"></a>日常工作</h2><p>之前提到过，我更偏向于Platform  SRE，偶尔扮演下业务SRE。我的工作占比大概是60%开发，40%运维，不过也不一定，有时候可能大部分时间是开发或者运维。开发的内容也比较多，涉及到各种语言，主要还是python和golang，偶尔还会写下前端。基本都是内部工具，比如基于开源项目做一些修改，像使用enovy给redis实现proxy，官方虽然有这个方案，但是某些命令不支持，这个时候就需要去实现。或者实现某些operator，以及魔改这些operator。 </p><p>运维方面主要是帮助开发解决中间件一些问题，简单的有为什么连接不上，可能是他们使用的不对，有时候比较困难的是为什么超时了，这个时候就需要排查各方面问题等等。以及修正监控之类的。</p><h3 id="其他人的工作内容"><a href="#其他人的工作内容" class="headerlink" title="其他人的工作内容"></a>其他人的工作内容</h3><p>像一些其他同事还需要参与值班和OnCall:</p><blockquote><p>Oncall 简单来说就是要保证线上服务的正常运行。典型的工作流程是：收到告警，检查告警发出的原因，确认线上服务是否有问题，定位到问题，解决问题。</p></blockquote><p>也需要去优化告警，有些时候可能是告警设置的不合理，就需要去调整告警阈值。</p><h3 id="都需要做的事情"><a href="#都需要做的事情" class="headerlink" title="都需要做的事情"></a>都需要做的事情</h3><h4 id="制定以及交付SLO和SLI"><a href="#制定以及交付SLO和SLI" class="headerlink" title="制定以及交付SLO和SLI"></a>制定以及交付SLO和SLI</h4><p>面对不同的场景交付的SLO也是不一样的。像生产集群可能是需要4个9，那SLI也会选的不一样。而开发和测试集群要求不高的话，3个9，关注点也不一样。在选定SLO的时候会考虑以下问题:</p><ol><li>如何定义这个可用率？</li><li>可用率计算的最小单位是什么？</li><li>可用率的周期是怎么计算的？</li><li>如何对 SLI 和 SLO 做监控？</li><li>如果错误预算即将用完，有什么措施？比如减少发布？如果 SLI 和 SLO 没有达到会怎么样</li></ol><h4 id="故障复盘"><a href="#故障复盘" class="headerlink" title="故障复盘"></a>故障复盘</h4><p>类似于b站这种对外的报告(<a href="https://mp.weixin.qq.com/s/nGtC5lBX_Iaj57HIdXq3Qg">https://mp.weixin.qq.com/s/nGtC5lBX_Iaj57HIdXq3Qg</a>)  也有在内部进行的，当然并不是所有的事故都会写复盘报告。一般情况下是生产集群或者发生严重事故(P0或者P1)就需要一个故障复盘来作为教训，避免下次有类似的情况发生。</p><h4 id="容量规划"><a href="#容量规划" class="headerlink" title="容量规划"></a>容量规划</h4><p>容量规划涉及到成本一块，如何为某个服务提供多少资源是一个问题，假设为某服务提供2Core2GRAM10GDisk，后续如果发生高并发或者突发情况，这个时候资源就不够用，就需要马上扩容。或者说给的资源太多，造成了浪费。所以容量规划是一个非常难做的事情。</p><h4 id="用户支持"><a href="#用户支持" class="headerlink" title="用户支持"></a>用户支持</h4><p>这里的用户除了买了服务这种，还有下游服务；这一块是技术咨询，以及用户要求的线上问题排查。日常需要写好文档，可能相同的问题会问个10遍20遍，如果有文档的话就可以方便的帮助用户解答。文档也需要经常更新。最好效果是通过文档就可以解决用户的问题。</p><h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><ul><li><a href="https://www.kawabangga.com/posts/4481">https://www.kawabangga.com/posts/4481</a></li><li><a href="https://tech.meituan.com/2017/08/03/meituanyun-sre.html">https://tech.meituan.com/2017/08/03/meituanyun-sre.html</a></li><li><a href="https://cloud.tencent.com/developer/article/1935721">https://cloud.tencent.com/developer/article/1935721</a></li></ul>]]></content>
    
    
    
    <tags>
      
      <tag>SRE</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Hello World</title>
    <link href="/2023/09/28/hello-world/"/>
    <url>/2023/09/28/hello-world/</url>
    
    <content type="html"><![CDATA[<p>Welcome to <a href="https://hexo.io/">Hexo</a>! This is your very first post. Check <a href="https://hexo.io/docs/">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a href="https://hexo.io/docs/troubleshooting.html">troubleshooting</a> or you can ask me on <a href="https://github.com/hexojs/hexo/issues">GitHub</a>.</p><h2 id="Quick-Start"><a href="#Quick-Start" class="headerlink" title="Quick Start"></a>Quick Start</h2><h3 id="Create-a-new-post"><a href="#Create-a-new-post" class="headerlink" title="Create a new post"></a>Create a new post</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">$ hexo new <span class="hljs-string">&quot;My New Post&quot;</span><br></code></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/writing.html">Writing</a></p><h3 id="Run-server"><a href="#Run-server" class="headerlink" title="Run server"></a>Run server</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">$ hexo server<br></code></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/server.html">Server</a></p><h3 id="Generate-static-files"><a href="#Generate-static-files" class="headerlink" title="Generate static files"></a>Generate static files</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">$ hexo generate<br></code></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/generating.html">Generating</a></p><h3 id="Deploy-to-remote-sites"><a href="#Deploy-to-remote-sites" class="headerlink" title="Deploy to remote sites"></a>Deploy to remote sites</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">$ hexo deploy<br></code></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/one-command-deployment.html">Deployment</a></p>]]></content>
    
    
    
  </entry>
  
  
  
  
</search>
